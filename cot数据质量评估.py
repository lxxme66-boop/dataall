# # -*- coding: utf-8 -*-
# # @Time : 2025/5/20 10:40 
# # @Author : dumingyu
# # @File : cotæ•°æ®è´¨é‡è¯„ä¼°.py
# # @Software: PyCharm


# import os
# import pandas as pd
# from openai import OpenAI
# import datetime
# import json
# import multiprocessing
# import logging
# from tqdm import tqdm

# # æ—¥å¿—é…ç½®
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# logger = logging.getLogger(__name__)

# time_label = str(datetime.datetime.today())[:10].replace('-', '')


# # åˆå§‹åŒ–å®¢æˆ·ç«¯
# client = OpenAI(
#     base_url="http://8.130.143.102:81/v1",  # vLLM API æœåŠ¡å™¨çš„åœ°å€
#     api_key="EMPTY"  # å¦‚æœæ²¡æœ‰è®¾ç½® API å¯†é’¥ï¼Œå¯ä»¥ä½¿ç”¨ä»»æ„å­—ç¬¦ä¸²
# )




# quality_evaluation_prompt_v1 = {
#     "role": "system",
#     "content": """
# ä½œä¸ºåŠå¯¼ä½“æ˜¾ç¤ºé¢†åŸŸçš„ä¸“ä¸šè´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ‡å‡†è¯„ä¼°é—®ç­”å¯¹çš„è´¨é‡ã€‚è¯„ä¼°åˆ†ä¸ºæ ¸å¿ƒç»´åº¦ï¼Œæ¯ä¸ªç»´åº¦åŒ…å«å…·ä½“è¯„ä¼°ç‚¹å’Œç¤ºä¾‹å‚è€ƒã€‚

# ### è¯„ä¼°ç»´åº¦
# 1. æ€ç»´é“¾é€»è¾‘è´¨é‡ï¼ˆæƒé‡35%ï¼‰
#    - æ­¥éª¤å®Œæ•´æ€§ï¼šæ¨ç†æ­¥éª¤æ˜¯å¦è¦†ç›–é—®é¢˜æ‰€æœ‰å…³é”®ç‚¹ï¼Ÿæ˜¯å¦é—æ¼å¿…è¦ç¯èŠ‚ï¼Ÿ
#    - å› æœè¿è´¯æ€§ï¼šå‰åæ­¥éª¤æ˜¯å¦å­˜åœ¨æ¸…æ™°å› æœå…³ç³»ï¼Ÿæœ‰æ— é€»è¾‘æ–­è£‚ï¼Ÿ
#    - æŠ€æœ¯å‚æ•°åˆç†æ€§ï¼šå·¥è‰ºå‚æ•°æ˜¯å¦ç¬¦åˆç‰©ç†è§„å¾‹ï¼Ÿï¼ˆä¾‹ï¼šLTPSé€€ç«æ¸©åº¦ä¸èƒ½è¶…è¿‡ç»ç’ƒè½¬åŒ–ç‚¹ï¼‰
#    - é”™è¯¯å›æº¯æœºåˆ¶ï¼šæ˜¯å¦è€ƒè™‘å¯èƒ½æ•…éšœç‚¹ï¼Ÿï¼ˆä¾‹ï¼šåˆ†æMuraç¼ºé™·åº”åŒ…å«è®¾å¤‡ç²¾åº¦å› ç´ ï¼‰

# 2. æŠ€æœ¯å‡†ç¡®åº¦ï¼ˆæƒé‡30%ï¼‰
#    - ææ–™ç‰¹æ€§ï¼šææ–™æè¿°æ˜¯å¦ç¬¦åˆç‰©æ€§ï¼Ÿï¼ˆä¾‹ï¼šIGZOè¿ç§»ç‡èŒƒå›´æ˜¯å¦æ­£ç¡®ï¼‰
#    - åˆ¶ç¨‹å‚æ•°ï¼šå·¥è‰ºå‚æ•°æ˜¯å¦åŒ¹é…è¡Œä¸šæ ‡å‡†ï¼Ÿï¼ˆä¾‹ï¼šå…‰åˆ»ç²¾åº¦æ˜¯å¦æ»¡è¶³å½“å‰äº§çº¿èƒ½åŠ›ï¼‰
#    - æ ‡å‡†å¼•ç”¨ï¼šæ˜¯å¦å‡†ç¡®å¼•ç”¨SEMI/SIDç­‰å›½é™…æ ‡å‡†ï¼Ÿ
#    - ä¸“åˆ©æŠ€æœ¯ï¼šæŠ€æœ¯æ–¹æ¡ˆæ˜¯å¦è§„é¿è¿‘æœŸä¸“åˆ©ï¼Ÿï¼ˆè‡ªåŠ¨æ ¸å¯¹2020-2024ä¸“åˆ©æ•°æ®åº“ï¼‰

# 3. é¢†åŸŸæ·±åº¦ï¼ˆæƒé‡20%ï¼‰
#    - ç¼ºé™·æœºç†ï¼šæ˜¯å¦åˆ†ææ ¹æœ¬åŸå› ï¼Ÿï¼ˆä¾‹ï¼šäº®æš—ç‚¹åº”å…³è”ç”µè‡´è¿ç§»æœºåˆ¶ï¼‰
#    - æŠ€æœ¯è¶‹åŠ¿ï¼šæ˜¯å¦è¦†ç›–æœ€æ–°å‘å±•ï¼Ÿï¼ˆä¾‹ï¼šéœ€æåŠMicro LEDå·¨é‡è½¬ç§»æŠ€æœ¯ï¼‰
#    - å·¥è‰ºç“¶é¢ˆï¼šæ˜¯å¦è¯†åˆ«å…³é”®é™åˆ¶ï¼Ÿï¼ˆä¾‹ï¼šæŒ‡å‡ºQD-OLEDçš„å–·å¢¨æ‰“å°ç²¾åº¦ç“¶é¢ˆï¼‰

# 4. åº”ç”¨ä»·å€¼ï¼ˆæƒé‡15%ï¼‰
#    - å·¥ç¨‹å¯è¡Œæ€§ï¼šæ–¹æ¡ˆæ˜¯å¦å…·å¤‡é‡äº§å®æ–½æ¡ä»¶ï¼Ÿ
#    - æˆæœ¬ä¼˜åŒ–ï¼šæ˜¯å¦é‡åŒ–æˆæœ¬æ•ˆç›Šï¼Ÿï¼ˆä¾‹ï¼šåº”è®¡ç®—é‡‡ç”¨MMGæŠ€æœ¯çš„ææ–™èŠ‚çœï¼‰
#    - è‰¯ç‡æå‡è·¯å¾„ï¼šæ˜¯å¦æä¾›å¯éªŒè¯çš„æ”¹å–„æ–¹æ¡ˆï¼Ÿ

# ### é¢†åŸŸå…³é”®ç‚¹ï¼ˆè‡ªåŠ¨æ ¸å¯¹ï¼‰
# | è¦ç´ ç±»å‹       | å…¸å‹å†…å®¹ç¤ºä¾‹                  |
# |----------------|------------------------------|
# | æ ¸å¿ƒææ–™       | æ°§åŒ–ç‰©TFT, QDææ–™, LTPO      |
# | å·¥è‰ºç—›ç‚¹       | è’¸é•€å‡åŒ€æ€§, æ°´æ°§é˜»éš”ç‡       |
# | å…¸å‹ç¼ºé™·       | Mura, äº®ç‚¹/æš—ç‚¹, çƒ­åº”åŠ›ç¿˜æ›²   |

# ### éªŒè¯æ–¹æ³•
# 1. å‚æ•°è¾¹ç•Œæ£€æŸ¥ï¼šå¯¹å…³é”®å‚æ•°è¿›è¡Œç‰©ç†æé™æ ¡éªŒï¼ˆä¾‹ï¼šè‹¥å£°ç§°PPI>1500éœ€éªŒè¯å…‰å­¦æ··è‰²è·ç¦»ï¼‰
# 2. æ—¶æ•ˆæ€§éªŒè¯ï¼šæŠ€æœ¯æŒ‡æ ‡æ˜¯å¦è¢«è¿‘3å¹´æ–‡çŒ®æ›´æ–°ï¼ˆè‡ªåŠ¨ç´¢å¼•IEEEæœŸåˆŠæ•°æ®åº“ï¼‰
# 3. æˆæœ¬åˆ†è§£ï¼šå¯¹é™æœ¬æ‰¿è¯ºè¿›è¡Œææ–™/è®¾å¤‡/è‰¯ç‡å› å­åˆ†è§£

# ### è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆJSONï¼‰
# {
#     "quality_rating": {
#         "overall": "high/medium/low",
#         "detailed_scores": {
#             "reasoning_chain": {"score": int, "issues": [str]},
#             "technical_accuracy": {"score": int, "validated": bool},
#             "domain_depth": {"score": int, "benchmark": str}
#         }
#     },
#     "improvement_suggestions": [str]
# }

# ### å¾…è¯„ä¼°æ ·æœ¬
# é—®é¢˜: {question_text}
# æ€ç»´é“¾: {reasoning_chain}
# ç­”æ¡ˆ: {answer_text}
# """
# }


# quality_evaluation_prompt_v2 = {
#     "role": "system",
#     "content": """ä½ æ˜¯ä¸€åèµ„æ·±æ˜¾ç¤ºæŠ€æœ¯é¢†åŸŸä¸“å®¶ã€‚è¯·ä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹æ˜¾ç¤ºæŠ€æœ¯ç›¸å…³çš„é—®ç­”å¯¹æ˜¯å¦é€‚åˆç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ•°æ®é›†æ„å»ºã€‚è¯„ä¼°éœ€åŸºäºä»¥ä¸‹å››ä¸ªæ ¸å¿ƒç»´åº¦ï¼š

# 1.  **å›ç­”ç›¸å…³æ€§ (Relevance)**ï¼šå›ç­”æ˜¯å¦ç²¾å‡†èšç„¦é—®é¢˜æ ¸å¿ƒï¼Ÿæ˜¯å¦å­˜åœ¨ç­”éæ‰€é—®ã€åç¦»ä¸»é¢˜æˆ–é—æ¼å…³é”®ç‚¹ï¼Ÿ
# 2.  **é€»è¾‘ä¸€è‡´æ€§ (Logical Consistency)**ï¼šå›ç­”çš„æ¨ç†è¿‡ç¨‹æ˜¯å¦æ¸…æ™°ã€è¿è´¯ã€æ— çŸ›ç›¾ï¼Ÿæ˜¯å¦å­˜åœ¨é€»è¾‘è·³è·ƒã€æ–­è£‚æˆ–è‡ªç›¸çŸ›ç›¾ï¼Ÿ
# 3.  **æœ¯è¯­ä½¿ç”¨ (Terminology Usage)**ï¼šä¸“ä¸šæœ¯è¯­çš„ä½¿ç”¨æ˜¯å¦å‡†ç¡®ã€æ°å½“ã€å®Œæ•´ï¼Ÿæ˜¯å¦å­˜åœ¨æœ¯è¯­è¯¯ç”¨ã€æ»¥ç”¨ã€ç¼ºå¤±æˆ–æ¦‚å¿µæ€§é”™è¯¯ï¼Ÿ
# 4.  **äº‹å®æ­£ç¡®æ€§ (Factual Correctness)**ï¼šå›ç­”ä¸­çš„æŠ€æœ¯ç»†èŠ‚ã€å‚æ•°ã€åŸç†ã€è¡Œä¸šç°çŠ¶ç­‰æ˜¯å¦ç¬¦åˆå·²çŸ¥äº‹å®å’Œè¡Œä¸šå…±è¯†ï¼Ÿæ˜¯å¦å­˜åœ¨äº‹å®æ€§é”™è¯¯æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Ÿ

# **æ€»ä½“è´¨é‡è¯„åˆ†æ ‡å‡†ï¼š**
# *   `low`ï¼š**å­˜åœ¨ä¸¥é‡ç¼ºé™·**ï¼ˆå¦‚æ˜æ˜¾äº‹å®é”™è¯¯ã€å®Œå…¨åç¦»ä¸»é¢˜ã€é€»è¾‘æ··ä¹±ã€å…³é”®æœ¯è¯­é”™è¯¯ï¼‰ï¼Œ**ä¸é€‚åˆ**ç”¨äºSFTã€‚
# *   `medium`ï¼š**å­˜åœ¨è½»å¾®é—®é¢˜æˆ–å¯ä¼˜åŒ–é¡¹**ï¼ˆå¦‚éƒ¨åˆ†è¡¨è¿°ä¸æ¸…ã€ä¸ªåˆ«æœ¯è¯­ä¸ä¸¥è°¨ã€æ¬¡è¦é€»è¾‘ä¸å®Œç¾ã€ç›¸å…³æ€§ç•¥æœ‰ä¸è¶³ï¼‰ï¼Œéœ€ä¿®æ”¹åæ–¹å¯è€ƒè™‘ä½¿ç”¨ã€‚
# *   `high`ï¼š**æ— æ˜æ˜¾é”™è¯¯**ï¼Œå†…å®¹**å‡†ç¡®ã€ä¸“ä¸šã€é€»è¾‘æ¸…æ™°ã€ç´§æ‰£ä¸»é¢˜**ï¼Œ**é€‚åˆ**ç›´æ¥ç”¨äºSFTã€‚

# **ä½ çš„ä»»åŠ¡ï¼š**
# 1.  å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œç‹¬ç«‹è¯„åˆ† (`high`/`medium`/`low`)ã€‚
# 2.  ç»™å‡ºåŸºäºå››ä¸ªç»´åº¦çš„**æ€»ä½“è´¨é‡è¯„åˆ†** (`high`/`medium`/`low`)ã€‚
# 3.  å¯¹äºè¯„åˆ†é`high`çš„ç»´åº¦ï¼Œ**å¿…é¡»å…·ä½“æŒ‡å‡º**å­˜åœ¨çš„é—®é¢˜åŠå…¶**ç±»å‹**ï¼ˆä¾‹å¦‚ï¼šâ€œæœ¯è¯­è¯¯ç”¨ï¼šå°†â€˜OLEDâ€™é”™è¯¯ç§°ä¸ºâ€˜LEDâ€™â€ï¼›â€œäº‹å®é”™è¯¯ï¼šå£°ç§°å½“å‰ä¸»æµMini-LEDèƒŒå…‰åˆ†åŒºæ•°æ™®éè¶…è¿‡5000åŒºâ€ï¼‰ã€‚
# 4.  åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œè¯„ä¼°ç»“æœï¼Œ**æä¾›å…·ä½“ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®**ï¼Œä»¥æå‡è¯¥é—®ç­”å¯¹çš„è´¨é‡ã€‚

# **è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆä¸¥æ ¼éµå¾ªJSONï¼‰ï¼š**
# {
#     "quality_rating": {
#         "overall": "high/medium/low", // æ€»ä½“è´¨é‡è¯„åˆ†
#         "detailed_scores": {
#             "Relevance": {"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜æè¿°1", "å…·ä½“é—®é¢˜æè¿°2", ...]}, // å¦‚æ— é—®é¢˜ï¼Œissuesä¸ºç©ºæ•°ç»„[]
#             "Logical Consistency": {"score": "high/medium/low", "issues": [...]},
#             "Terminology Usage": {"score": "high/medium/low", "issues": [...]},
#             "Factual Correctness": {"score": "high/medium/low", "issues": [...]}
#         }
#     },
#     "improvement_suggestions": ["å…·ä½“å»ºè®®1", "å…·ä½“å»ºè®®2", ...] // å³ä½¿æ€»ä½“æ˜¯highï¼Œä¹Ÿå¯æä¾›ä¼˜åŒ–å»ºè®®
# }
# ### å¾…è¯„ä¼°æ ·æœ¬
# é—®é¢˜: {question_text}
# æ€ç»´é“¾: {reasoning_chain}
# ç­”æ¡ˆ: {answer_text}
# """
# }


# # å‘é€å•ä¸ªè¯·æ±‚
# def evaluate_qa_quality(question, chain, answer):
#     try:
#         prompt = quality_evaluation_prompt.copy()
#         prompt["content"] = prompt["content"].replace("{question_text}", question) \
#             .replace("{reasoning_chain}", chain) \
#             .replace("{answer_text}", answer)

#         response = client.chat.completions.create(
#             model="qwen3-235b",  # ä¸ --served-model-name å‚æ•°æŒ‡å®šçš„åç§°ä¸€è‡´
#             messages=[prompt],
#             temperature=0.1
#         )
#         # æ‰“å°æ¨¡å‹çš„å“åº”
#         return response.choices[0].message.content
#     except Exception as e:
#         return f"è¯·æ±‚å¤±è´¥: {str(e)}"


# # ================ å¤šè¿›ç¨‹å·¥ä½œå‡½æ•° ================
# def process_row(args):
#     """
#     å¤„ç†å•ä¸ªæ•°æ®è¡Œçš„è´¨é‡è¯„ä¼°ä»»åŠ¡
#     """
#     row_idx, row_data, server_config = args
#     question, chain, answer = row_data['é—®é¢˜'], row_data['æ€ç»´é“¾'], row_data['ç­”æ¡ˆ']

#     # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆæ¯ä¸ªè¿›ç¨‹å•ç‹¬åˆ›å»ºï¼‰
#     client = OpenAI(
#         base_url=server_config["base_url"],
#         api_key=server_config["api_key"]
#     )

#     try:
#         # 2. æ„å»ºè¯„ä¼°æç¤º
#         prompt = quality_evaluation_prompt.copy()
#         prompt["content"] = prompt["content"].replace("{question_text}", question) \
#             .replace("{reasoning_chain}", chain) \
#             .replace("{answer_text}", answer)

#         # 3. å‘é€è¯·æ±‚
#         response = client.chat.completions.create(
#             model=server_config["model_name"],
#             messages=[prompt],
#             timeout=90  # è®¾ç½®è¶…æ—¶é˜²æ­¢è¿›ç¨‹å¡ä½
#         )


#         # 4. è§£æå“åº”
#         result_text = response.choices[0].message.content
#         if '```json' in result_text:
#             result = json.loads(result_text.split("```json")[-1].strip().replace("```", "").strip())
#         else:
#             result = json.loads(result_text.split("</think>")[-1].strip().replace("```", "").strip())
#         # 5. æ›´æ–°è¡Œæ•°æ®
#         row_data['quality_rating'] = result['quality_rating']['overall']
#         row_data['detailed_scores'] = str(result['quality_rating']['detailed_scores'])
#         return (row_idx, row_data)

#     except Exception as e:
#         logger.error(f"å¤„ç†è¡Œ {row_idx} æ—¶å‡ºé”™: {str(e)}")
#         # ä¿ç•™åŸå§‹è¡Œæ•°æ®å¹¶æ ‡è®°é”™è¯¯
#         row_data['quality_rating'] = "ERROR"
#         row_data['detailed_scores'] = str(e)
#         return (row_idx, row_data)





# if __name__ == '__main__':

#     # 0. æœåŠ¡å™¨é…ç½®ï¼ˆé¿å…é‡å¤ä¼ é€’ï¼‰
#     server_config = {
#         "base_url": "http://8.130.143.102:81/v1",
#         "api_key": "EMPTY",
#         "model_name": "qwen3-235b"
#     }

#     # 1. åŠ è½½æºæ•°æ®
#     file_name = "/mnt/workspace/LLM/ldd/sft/è¯„ä¼°æ•°æ®è¾“å…¥.xlsx"
#     data = pd.read_excel(file_name)
#     logger.info(f"åŠ è½½æ•°æ®å®Œæˆï¼Œå…± {data.shape[0]} è¡Œ")

#     # 2. å‡†å¤‡å¤šè¿›ç¨‹å¤„ç†
#     num_workers = 20
#     logger.info(f"å¯ç”¨ {num_workers} ä¸ªå·¥ä½œè¿›ç¨‹")


#     data = data.iloc[10848:,]


#     # 3. åˆ›å»ºè¿›ç¨‹æ± 
#     with multiprocessing.Pool(processes=num_workers) as pool:
#         # å‡†å¤‡ä»»åŠ¡å‚æ•° [(è¡Œç´¢å¼•, è¡Œæ•°æ®, æœåŠ¡å™¨é…ç½®), ...]
#         task_args = [(idx, row.copy(), server_config) for idx, row in data.iterrows()]

#         # 4. å¹¶è¡Œå¤„ç†å¹¶æ”¶é›†ç»“æœ
#         results = []
#         for result in tqdm(pool.imap(process_row, task_args), total=len(task_args)):
#             results.append(result)

#     # 5. æŒ‰åŸå§‹é¡ºåºæ›´æ–°æ•°æ®
#     logger.info("å¼€å§‹æ›´æ–°ç»“æœæ•°æ®")
#     for row_idx, updated_row in results:
#         data.loc[row_idx,'quality_rating'] = updated_row['quality_rating']
#         data.loc[row_idx, 'detailed_scores'] = updated_row['detailed_scores']

#     # 6. ä¿å­˜ç»“æœ
#     save_dir = './reslut'
#     save_name = 'å¢å¼ºå‰çš„è¯„ä¼°ç»“æœ.xlsx'
#     save_path = os.path.join(save_dir, save_name)

#     with pd.ExcelWriter(save_path) as writer:
#         data.to_excel(writer, index=False)
#     logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {save_path}")

# -*- coding: utf-8 -*-
# @Time : 2025/5/20 10:40 
# @Author : dumingyu
# @File : cotæ•°æ®è´¨é‡è¯„ä¼°.py
# @Software: PyCharm

# -*- coding: utf-8 -*-
# @Time : 2025/5/20 10:40 
# @Author : dumingyu
# @File : cotæ•°æ®è´¨é‡è¯„ä¼°.py
# @Software: PyCharm

# -*- coding: utf-8 -*-
# @Time : 2025/5/20 10:40 
# @Author : dumingyu
# @File : cotæ•°æ®è´¨é‡è¯„ä¼°.py
# @Software: PyCharm

# -*- coding: utf-8 -*-
# @Time : 2025/5/20 10:40 
# @Author : dumingyu
# @File : cotæ•°æ®è´¨é‡è¯„ä¼°.py
# @Software: PyCharm


# #å¸¦æ–‡æœ¬
# import os
# import pandas as pd
# from openai import OpenAI
# import datetime
# import json
# import multiprocessing
# import logging
# from tqdm import tqdm
# import time
# import requests
# import re
# import traceback

# # æ—¥å¿—é…ç½®
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# logger = logging.getLogger(__name__)

# time_label = str(datetime.datetime.today())[:10].replace('-', '')

# # ä½¿ç”¨v2ç‰ˆæœ¬çš„è¯„ä¼°æç¤ºæ¨¡æ¿
# quality_evaluation_prompt = {
#     "role": "system",
#     "content": """ä½ æ˜¯ä¸€åèµ„æ·±æ˜¾ç¤ºæŠ€æœ¯é¢†åŸŸä¸“å®¶ã€‚è¯·å…ˆä»”ç»†æ€è€ƒï¼Œç„¶åä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹æ˜¾ç¤ºæŠ€æœ¯ç›¸å…³çš„é—®ç­”å¯¹æ˜¯å¦é€‚åˆç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ•°æ®é›†æ„å»ºã€‚
# [é—®é¢˜]ï¼š
# {question_text}
# [æ–‡æœ¬]ï¼š
# {reasoning_text}
# [ç­”æ¡ˆ]ï¼š
# {answer_text}
# ##æ ¸å¿ƒç»´åº¦ï¼Œè¯„ä¼°éœ€åŸºäºä»¥ä¸‹å…­ä¸ªæ ¸å¿ƒç»´åº¦ï¼š

# 0.  **é—®é¢˜é€šç”¨æ€§(quedtion)**: é—®é¢˜æ˜¯å¦æ˜¯é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆçš„ï¼Ÿæ˜¯å¦èƒ½æ€ä¹ˆæ–‡æœ¬å›ç­”ï¼Ÿé—®é¢˜æ˜¯å¦å…·æœ‰å®é™…æ„ä¹‰ï¼Ÿé—®é¢˜æ˜¯å¦é€šç”¨ï¼Ÿï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
# 1.  **å›ç­”ç›¸å…³æ€§ (Relevance)**ï¼šå›ç­”æ˜¯å¦ç²¾å‡†èšç„¦é—®é¢˜æ ¸å¿ƒï¼Ÿæ˜¯å¦å­˜åœ¨ç­”éæ‰€é—®ã€åç¦»ä¸»é¢˜æˆ–é—æ¼å…³é”®ç‚¹ï¼Ÿæ˜¯å¦ç­”æ¡ˆåªæ˜¯ä»…å¼•å¯¼å¥æœªæä¾›å®è´¨æ€§å†…å®¹ï¼Ÿç­”æ¡ˆæ˜¯å¦åŸºäºè®ºæ–‡åŸæ–‡å›ç­”çš„ï¼Œå³æ˜¯å¦èƒ½åœ¨ä¸Šä¼ çš„æœ¬æ–‡ä¸­æ‰¾åˆ°å‡ºå¤„ï¼Ÿï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
# 2.  **é€»è¾‘ä¸€è‡´æ€§ (Logical Consistency)**ï¼šå›ç­”çš„æ¨ç†è¿‡ç¨‹æ˜¯å¦æ¸…æ™°ã€è¿è´¯ã€æ— çŸ›ç›¾ï¼Ÿæ˜¯å¦å­˜åœ¨é€»è¾‘è·³è·ƒã€æ–­è£‚æˆ–è‡ªç›¸çŸ›ç›¾ï¼Ÿæ˜¯å¦å­˜åœ¨ç­”æ¡ˆä¸­æ–­ï¼Ÿç­”æ¡ˆæ˜¯å¦å’Œé—®é¢˜ä¸ç›¸å…³ï¼Ÿç­”æ¡ˆæ˜¯å¦åªæ˜¯æ³›æ³›è€Œè°ˆï¼Ÿ
# 3.  **æœ¯è¯­ä½¿ç”¨ (Terminology Usage)**ï¼šä¸“ä¸šæœ¯è¯­çš„ä½¿ç”¨æ˜¯å¦å‡†ç¡®ã€æ°å½“ã€å®Œæ•´ï¼Ÿæ˜¯å¦å­˜åœ¨æœ¯è¯­è¯¯ç”¨ã€æ»¥ç”¨ã€ç¼ºå¤±æˆ–æ¦‚å¿µæ€§é”™è¯¯ï¼Ÿ
# 4.  **äº‹å®æ­£ç¡®æ€§ (Factual Correctness)**ï¼šå›ç­”ä¸­çš„æŠ€æœ¯ç»†èŠ‚ã€å‚æ•°ã€åŸç†ã€è¡Œä¸šç°çŠ¶ç­‰æ˜¯å¦ç¬¦åˆå·²çŸ¥äº‹å®å’Œè¡Œä¸šå…±è¯†ï¼Ÿæ˜¯å¦å­˜åœ¨äº‹å®æ€§é”™è¯¯æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Ÿæ˜¯å¦å…·æœ‰é€šç”¨æ€§ï¼Ÿ
# 5.  **åŸæ–‡æœ¬éªŒè¯(validate against the original text)**:ç­”æ¡ˆä¸­æ ‡è®°äº†è¯¥ç­”æ¡ˆçš„å‡ºå¤„ï¼Œè¯·ä»”ç»†é˜…è¯»åŸæ–‡å’Œç­”æ¡ˆå‡ºå¤„ï¼Œæ€è€ƒç­”æ¡ˆå‡ºå¤„æ˜¯å¦çœŸçš„æ¥è‡ªåŸæ–‡ï¼Ÿ(ä¸¥æ ¼æ‰§è¡Œ)

# ##æ€»ä½“è´¨é‡è¯„åˆ†æ ‡å‡†ï¼š
# *   `low`ï¼š**å­˜åœ¨ä¸¥é‡ç¼ºé™·**ï¼ˆå¦‚é—®é¢˜é€šç”¨æ€§ä½è´¨é‡ã€æ˜æ˜¾äº‹å®é”™è¯¯ã€å®Œå…¨åç¦»ä¸»é¢˜ã€é€»è¾‘æ··ä¹±ã€å…³é”®æœ¯è¯­é”™è¯¯ã€ç­”æ¡ˆä¸å®Œæ•´ï¼ˆä»…å¼•å¯¼å¥ï¼‰æœªæä¾›å®è´¨æ€§å†…å®¹ï¼ˆç­”æ¡ˆå®Œæ•´æ€§ä¸è¿‡å…³ï¼‰ã€ç­”æ¡ˆä¸æ˜¯å‡ºè‡ªåŸæ–‡ï¼ˆä¸èƒ½åœ¨åŸæ–‡ä¸­æ‰¾åˆ°å‡ºå¤„ï¼‰ã€ç­”æ¡ˆä¸­æ ‡è®°çš„å‡ºå¤„ä¸æ˜¯å‡ºåœ¨åŸæ–‡ï¼‰ï¼Œ**ä¸é€‚åˆ**ç”¨äºSFT,ã€‚
# *   `medium`ï¼š**å­˜åœ¨è½»å¾®é—®é¢˜æˆ–å¯ä¼˜åŒ–é¡¹**ï¼ˆå¦‚éƒ¨åˆ†è¡¨è¿°ä¸æ¸…ã€ä¸ªåˆ«æœ¯è¯­ä¸ä¸¥è°¨ã€æ¬¡è¦é€»è¾‘ä¸å®Œç¾ã€ç›¸å…³æ€§ç•¥æœ‰ä¸è¶³ã€é€šç”¨æ€§ç•¥æœ‰ä¸è¶³ã€ä¸»è¦é€»è¾‘åŸºäºè®ºæ–‡å›ç­”ã€å‡ºå¤„çš„æ ¸å¿ƒé€»è¾‘å‡ºè‡ªåŸæ–‡ï¼‰ï¼Œéœ€ä¿®æ”¹åæ–¹å¯è€ƒè™‘ä½¿ç”¨ã€‚
# *   `high`ï¼š**æ— æ˜æ˜¾é”™è¯¯**ï¼Œå†…å®¹**(é—®é¢˜é€šç”¨æ€§é«˜è´¨é‡ã€å‡†ç¡®ã€ä¸“ä¸šã€é€»è¾‘æ¸…æ™°ã€ç´§æ‰£ä¸»é¢˜ã€å‡†ç¡®ä¸”å®Œæ•´åœ°å›ç­”äº†é—®é¢˜ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ã€åŸºäºè®ºæ–‡å›ç­”ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ã€ç­”æ¡ˆä¸­çš„æ ‡è®°çš„å‡ºå¤„å®Œå…¨å‡ºè‡ªåŸæ–‡ã€å…·æœ‰é€šç”¨æ€§)**ï¼Œ**é€‚åˆ**ç›´æ¥ç”¨äºSFTã€‚

# ##ä½ çš„ä»»åŠ¡ï¼š
# 1.  å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œç‹¬ç«‹è¯„åˆ† (`high`/`medium`/`low`)ã€‚
# 2.  ç»™å‡ºåŸºäºå…­ä¸ªç»´åº¦å’Œæ–°åŠ è¦æ±‚çš„**æ€»ä½“è´¨é‡è¯„åˆ†** (`high`/`medium`/`low`)ï¼Œå…¶ä¸­è‹¥ç­”æ¡ˆå®Œæ•´æ€§ä¸è¿‡å…³ï¼ˆåªæ˜¯ä»…å¼•å¯¼å¥æœªæä¾›å®è´¨æ€§å†…å®¹ï¼‰ã€ç­”æ¡ˆå®Œå…¨ä¸æ˜¯åŸºäºåŸæ–‡å›ç­”çš„ã€åŸæ–‡éªŒè¯ä¸­çš„å‡ºå¤„å®é™…å’ŒåŸæ–‡ä¸æ˜¯å¾ˆç›¸å…³ã€ç­”æ¡ˆä¸ºæ— æ³•ä½œç­”ã€ç­”æ¡ˆä¸­ç­”æ¡ˆå‡ºå¤„æ²¡æœ‰ã€é—®é¢˜é€šç”¨æ€§ä¸è¿‡å…³ï¼Œæ»¡è¶³å…¶ä¸€ç›´æ¥ä¸€ç¥¨å¦å†³ï¼Œåˆ¤ä¸ºä½è´¨é‡ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ã€‚
# 3.  å¯¹äºè¯„åˆ†é`high`çš„ç»´åº¦ï¼Œ**å¿…é¡»å…·ä½“æŒ‡å‡º**å­˜åœ¨çš„é—®é¢˜åŠå…¶**ç±»å‹**ï¼ˆä¾‹å¦‚ï¼š"æœ¯è¯­è¯¯ç”¨ï¼šå°†'OLED'é”™è¯¯ç§°ä¸º'LED'"ï¼›"äº‹å®é”™è¯¯ï¼šå£°ç§°å½“å‰ä¸»æµMini-LEDèƒŒå…‰åˆ†åŒºæ•°æ™®éè¶…è¿‡5000åŒº"ï¼‰ã€‚
# 4.  åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œè¯„ä¼°ç»“æœï¼Œ**æä¾›å…·ä½“ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®**ï¼Œä»¥æå‡è¯¥é—®ç­”å¯¹çš„è´¨é‡ã€‚

# #è¾“å‡ºæ ¼å¼è¦æ±‚(ä¸¥æ ¼éµå¾ªJSON):
# {
#     "quality_rating": {
#         "overall": "high/medium/low", // æ€»ä½“è´¨é‡è¯„åˆ†
#         "detailed_scores": {
#             "Relevance": {"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜æè¿°1", "å…·ä½“é—®é¢˜æè¿°2", ...]}, // å¦‚æ— é—®é¢˜ï¼Œissuesä¸ºç©ºæ•°ç»„[]
#             "Logical Consistency": {"score": "high/medium/low", "issues": [...]},
#             "Terminology Usage": {"score": "high/medium/low", "issues": [...]},
#             "Factual Correctness": {"score": "high/medium/low", "issues": [...]}
#             "validate against the original text":{"score": "high/medium/low", "issues": [...]}
#         }
#     },
#     "improvement_suggestions": ["å…·ä½“å»ºè®®1", "å…·ä½“å»ºè®®2", ...] // å³ä½¿æ€»ä½“æ˜¯highï¼Œä¹Ÿå¯æä¾›ä¼˜åŒ–å»ºè®®
# }


# """
# }


# # ================ å¤šè¿›ç¨‹å·¥ä½œå‡½æ•° ================
# def process_row(args):
#     """
#     å¤„ç†å•ä¸ªæ•°æ®è¡Œçš„è´¨é‡è¯„ä¼°ä»»åŠ¡
#     """
#     row_idx, row_data, server_config = args
#     # ä½¿ç”¨å®é™…è¡¨æ ¼ä¸­çš„åˆ—å
#     question = row_data['é—®é¢˜'] if 'é—®é¢˜' in row_data else row_data.get('é—®é¢˜"', '')
#     chain = row_data['æ€ç»´é“¾'] if 'æ€ç»´é“¾' in row_data else row_data.get('ä½¿ç”¨CoT', '')
#     answer = row_data['ç­”æ¡ˆ'] if 'ç­”æ¡ˆ' in row_data else row_data.get('ç­”æ¡ˆ"', '')
    
#     # å¤„ç†å¯èƒ½çš„ç©ºå€¼
#     question = str(question) if pd.notna(question) else ""
#     chain = str(chain) if pd.notna(chain) else ""
#     answer = str(answer) if pd.notna(answer) else ""

#     # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆæ¯ä¸ªè¿›ç¨‹å•ç‹¬åˆ›å»ºï¼‰
#     client = OpenAI(
#         base_url=server_config["base_url"],
#         api_key=server_config["api_key"]
#     )

#     try:
#         # 2. æ„å»ºè¯„ä¼°æç¤º
#         prompt = quality_evaluation_prompt.copy()
#         prompt["content"] = prompt["content"].replace("{question_text}", question) \
#             .replace("{reasoning_text}", chain) \
#             .replace("{answer_text}", answer)

#         # 3. å‘é€è¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
#         max_retries = 5
#         retry_delay = 5  # åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
#         response = None
        
#         for attempt in range(max_retries):
#             try:
#                 response = client.chat.completions.create(
#                     model=server_config["model_name"],
#                     messages=[prompt],
#                     timeout=300  # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
#                 )
#                 break  # è¯·æ±‚æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
#             except Exception as e:
#                 if attempt < max_retries - 1:
#                     logger.warning(f"è¡Œ {row_idx} è¯·æ±‚å¤±è´¥ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•: {str(e)}")
#                     time.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿ç­–ç•¥
#                 else:
#                     raise e  # æœ€åä¸€æ¬¡é‡è¯•ä»ç„¶å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸

#         # 4. è§£æå“åº”
#         result_text = response.choices[0].message.content
        
#         # å¢å¼ºJSONè§£æ - å¤„ç†å„ç§å¯èƒ½çš„æ ¼å¼
#         result = None
#         try:
#             # å°è¯•1: ç›´æ¥è§£ææ•´ä¸ªå“åº”
#             result = json.loads(result_text)
#         except json.JSONDecodeError:
#             # å°è¯•2: æå–å¯èƒ½çš„JSONéƒ¨åˆ†
#             try:
#                 # å¤„ç†ä»£ç å—æ ¼å¼
#                 if '```json' in result_text:
#                     json_str = re.search(r'```json(.*?)```', result_text, re.DOTALL)
#                     if json_str:
#                         json_str = json_str.group(1).strip()
#                         result = json.loads(json_str)
#                 # å¤„ç†æ€è€ƒæ ¼å¼
#                 elif '</think>' in result_text:
#                     json_str = result_text.split('</think>', 1)[1].strip()
#                     result = json.loads(json_str)
#                 # å°è¯•æå–JSONå¯¹è±¡
#                 else:
#                     json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
#                     if json_match:
#                         json_str = json_match.group(0).strip()
#                         # å¤„ç†å¯èƒ½çš„å¤šä¸ªJSONå¯¹è±¡
#                         if json_str.count('{') > 1:
#                             # å–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
#                             start_idx = json_str.find('{')
#                             end_idx = json_str.rfind('}', 0, json_str.find('}', start_idx) + 1)
#                             json_str = json_str[start_idx:end_idx+1]
#                         result = json.loads(json_str)
#             except:
#                 # å°è¯•3: ä½œä¸ºæœ€åçš„æ‰‹æ®µï¼Œæå–JSONéƒ¨åˆ†
#                 try:
#                     start_idx = result_text.find('{')
#                     end_idx = result_text.rfind('}')
#                     if start_idx >= 0 and end_idx > start_idx:
#                         json_str = result_text[start_idx:end_idx+1]
#                         result = json.loads(json_str)
#                 except Exception as e:
#                     logger.error(f"JSONè§£æå¤±è´¥ï¼Œå“åº”æ–‡æœ¬: {result_text[:500]}...")
#                     raise ValueError(f"æ— æ³•è§£æJSONå“åº”: {str(e)}")
        
#         # 5. æ›´æ–°è¡Œæ•°æ®
#         if result and 'quality_rating' in result and 'overall' in result['quality_rating']:
#             row_data['quality_rating'] = result['quality_rating']['overall']
#             row_data['detailed_scores'] = str(result['quality_rating'].get('detailed_scores', ''))
#             row_data['improvement_suggestions'] = str(result.get('improvement_suggestions', []))
#         else:
#             raise ValueError("å“åº”ä¸­ç¼ºå°‘å¿…è¦çš„quality_ratingå­—æ®µ")
            
#         return (row_idx, row_data)

#     except Exception as e:
#         logger.error(f"å¤„ç†è¡Œ {row_idx} æ—¶å‡ºé”™: {str(e)}")
#         logger.debug(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
#         # ä¿ç•™åŸå§‹è¡Œæ•°æ®å¹¶æ ‡è®°é”™è¯¯
#         row_data['quality_rating'] = "ERROR"
#         row_data['detailed_scores'] = str(e)
#         return (row_idx, row_data)

# def check_server_health(base_url):
#     """æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å¯ç”¨"""
#     health_url = base_url.replace("/v1", "/health")
#     try:
#         response = requests.get(health_url, timeout=10)
#         if response.status_code == 200:
#             logger.info(f"æœåŠ¡å™¨å¥åº·æ£€æŸ¥é€šè¿‡: {health_url}")
#             return True
#         else:
#             logger.warning(f"æœåŠ¡å™¨å¥åº·æ£€æŸ¥å¤±è´¥: çŠ¶æ€ç  {response.status_code}")
#             return False
#     except Exception as e:
#         logger.error(f"æœåŠ¡å™¨å¥åº·æ£€æŸ¥é”™è¯¯: {str(e)}")
#         return False

# def extract_json_from_response(text):
#     """ä»å“åº”æ–‡æœ¬ä¸­æå–JSONå†…å®¹"""
#     # å°è¯•ç›´æ¥è§£æ
#     try:
#         return json.loads(text)
#     except:
#         pass
    
#     # å°è¯•æå–ä»£ç å—
#     if '```json' in text:
#         try:
#             json_match = re.search(r'```json(.*?)```', text, re.DOTALL)
#             if json_match:
#                 return json.loads(json_match.group(1).strip())
#         except:
#             pass
    
#     # å°è¯•æå–æ€è€ƒå—
#     if '</think>' in text:
#         try:
#             json_part = text.split('</think>', 1)[1].strip()
#             return json.loads(json_part)
#         except:
#             pass
    
#     # å°è¯•æå–JSONå¯¹è±¡
#     try:
#         json_match = re.search(r'\{.*\}', text, re.DOTALL)
#         if json_match:
#             return json.loads(json_match.group(0).strip())
#     except:
#         pass
    
#     # ä½œä¸ºæœ€åæ‰‹æ®µï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªJSONå¯¹è±¡
#     try:
#         start_idx = text.find('{')
#         end_idx = text.rfind('}')
#         if start_idx >= 0 and end_idx > start_idx:
#             return json.loads(text[start_idx:end_idx+1])
#     except:
#         pass
    
#     raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„JSON")

# if __name__ == '__main__':
#     # 0. æœåŠ¡å™¨é…ç½® - ä½¿ç”¨vLLM HTTPé…ç½®
#     server_config = {
#         "base_url": os.getenv("VLLM_SERVER_URL", "http://localhost:8000/v1"),
#         "api_key": "EMPTY",
#         "model_name": os.getenv("VLLM_MODEL_NAME", "qwen-vllm")
#     }
    
#     logger.info(f"ä½¿ç”¨vLLMæœåŠ¡å™¨: {server_config['base_url']}")
#     logger.info(f"ä½¿ç”¨æ¨¡å‹: {server_config['model_name']}")
    
#     # æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€
#     if not check_server_health(server_config["base_url"]):
#         logger.error("æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥vLLMæœåŠ¡æ˜¯å¦å·²å¯åŠ¨")
#         exit(1)

#     # 1. åŠ è½½æºæ•°æ®
#     file_name = "/mnt/workspace/LLM/ldd/sft/input/Qwen325è¯„ä¼°æ•°æ®v2ç‰ˆè¾“å…¥-æ— å¢å¼º.xlsx"
#     print(file_name)
#     data = pd.read_excel(file_name)
#     logger.info(f"åŠ è½½æ•°æ®å®Œæˆï¼Œå…± {data.shape[0]} è¡Œ")
#     logger.info(f"æ•°æ®åˆ—å: {list(data.columns)}")
    
#     # æ·»åŠ ç»“æœåˆ—
#     data['quality_rating'] = ""
#     data['detailed_scores'] = ""
#     data['improvement_suggestions'] = ""

#     # 2. å‡†å¤‡å¤šè¿›ç¨‹å¤„ç†
#     num_workers = min(8, os.cpu_count())  # å‡å°‘å¹¶å‘æ•°ä»¥æé«˜ç¨³å®šæ€§
#     logger.info(f"å¯ç”¨ {num_workers} ä¸ªå·¥ä½œè¿›ç¨‹")

#     # 3. åˆ›å»ºè¿›ç¨‹æ± 
#     with multiprocessing.Pool(processes=num_workers) as pool:
#         # å‡†å¤‡ä»»åŠ¡å‚æ•° [(è¡Œç´¢å¼•, è¡Œæ•°æ®, æœåŠ¡å™¨é…ç½®), ...]
#         task_args = [(idx, row.copy(), server_config) for idx, row in data.iterrows()]

#         # 4. å¹¶è¡Œå¤„ç†å¹¶æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
#         results = []
#         progress_bar = tqdm(total=len(task_args), desc="è¯„ä¼°è¿›åº¦", unit="è¡Œ")
        
#         try:
#             for result in pool.imap(process_row, task_args):
#                 results.append(result)
#                 row_idx, row_data = result
#                 progress_bar.update(1)
#                 progress_bar.set_postfix_str(f"çŠ¶æ€: {row_data['quality_rating']}")
#         except Exception as e:
#             logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
#             progress_bar.close()
#             pool.terminate()  # ç»ˆæ­¢æ‰€æœ‰è¿›ç¨‹
#             raise e
        
#         progress_bar.close()

#     # 5. æŒ‰åŸå§‹é¡ºåºæ›´æ–°æ•°æ®
#     logger.info("å¼€å§‹æ›´æ–°ç»“æœæ•°æ®")
#     for row_idx, updated_row in results:
#         data.loc[row_idx, 'quality_rating'] = updated_row['quality_rating']
#         data.loc[row_idx, 'detailed_scores'] = updated_row['detailed_scores']
#         data.loc[row_idx, 'improvement_suggestions'] = updated_row.get('improvement_suggestions', '')

#     # 6. ä¿å­˜ç»“æœ
#     save_dir = './result'
#     os.makedirs(save_dir, exist_ok=True)
#     save_name = f'æ•°æ®è´¨é‡è¯„ä¼°ç»“æœ_{time_label}â€”v2æ— å¢å¼ºç‰ˆ.xlsx'
#     save_path = os.path.join(save_dir, save_name)

#     with pd.ExcelWriter(save_path) as writer:
#         data.to_excel(writer, index=False)
#     logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    
#     # æ‰“å°æ‘˜è¦ç»Ÿè®¡
#     if 'quality_rating' in data.columns:
#         rating_counts = data['quality_rating'].value_counts()
#         logger.info("\n===== è¯„ä¼°ç»“æœæ‘˜è¦ =====")
#         logger.info(f"é«˜è´¨é‡ (high): {rating_counts.get('high', 0)} è¡Œ")
#         logger.info(f"ä¸­ç­‰è´¨é‡ (medium): {rating_counts.get('medium', 0)} è¡Œ")
#         logger.info(f"ä½è´¨é‡ (low): {rating_counts.get('low', 0)} è¡Œ")
#         logger.info(f"é”™è¯¯è¡Œ: {rating_counts.get('ERROR', 0)} è¡Œ")
#     else:
#         logger.warning("æœªæ‰¾åˆ°è´¨é‡è¯„çº§åˆ—ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡")


# ----------------è€ƒè™‘æ–‡æœ¬
# -*- coding: utf-8 -*-
# @Time : 2025/5/20 10:40 
# @Author : dumingyu
# @File : cotæ•°æ®è´¨é‡è¯„ä¼°_ç«å±±ç‰ˆ.py
# @Software: PyCharm

import os
import pandas as pd
from volcenginesdkarkruntime import Ark
import datetime
import json
import multiprocessing
import logging
from tqdm import tqdm

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

time_label = str(datetime.datetime.today())[:10].replace('-', '')

# åˆå§‹åŒ–ç«å±±å¼•æ“å®¢æˆ·ç«¯
client = Ark(
    api_key="5a032496-1268-4e6f-b6ee-a9affc6b5469",
    base_url="https://ark.cn-beijing.volces.com/api/v3",
)

quality_evaluation_prompt_v1 = {
    "role": "system",
    "content": """
ä½œä¸ºåŠå¯¼ä½“æ˜¾ç¤ºé¢†åŸŸçš„ä¸“ä¸šè´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ‡å‡†è¯„ä¼°é—®ç­”å¯¹çš„è´¨é‡ã€‚è¯„ä¼°åˆ†ä¸ºæ ¸å¿ƒç»´åº¦ï¼Œæ¯ä¸ªç»´åº¦åŒ…å«å…·ä½“è¯„ä¼°ç‚¹å’Œç¤ºä¾‹å‚è€ƒã€‚

### è¯„ä¼°ç»´åº¦
1. æ€ç»´é“¾é€»è¾‘è´¨é‡ï¼ˆæƒé‡35%ï¼‰
   - æ­¥éª¤å®Œæ•´æ€§ï¼šæ¨ç†æ­¥éª¤æ˜¯å¦è¦†ç›–é—®é¢˜æ‰€æœ‰å…³é”®ç‚¹ï¼Ÿæ˜¯å¦é—æ¼å¿…è¦ç¯èŠ‚ï¼Ÿ
   - å› æœè¿è´¯æ€§ï¼šå‰åæ­¥éª¤æ˜¯å¦å­˜åœ¨æ¸…æ™°å› æœå…³ç³»ï¼Ÿæœ‰æ— é€»è¾‘æ–­è£‚ï¼Ÿ
   - æŠ€æœ¯å‚æ•°åˆç†æ€§ï¼šå·¥è‰ºå‚æ•°æ˜¯å¦ç¬¦åˆç‰©ç†è§„å¾‹ï¼Ÿï¼ˆä¾‹ï¼šLTPSé€€ç«æ¸©åº¦ä¸èƒ½è¶…è¿‡ç»ç’ƒè½¬åŒ–ç‚¹ï¼‰
   - é”™è¯¯å›æº¯æœºåˆ¶ï¼šæ˜¯å¦è€ƒè™‘å¯èƒ½æ•…éšœç‚¹ï¼Ÿï¼ˆä¾‹ï¼šåˆ†æMuraç¼ºé™·åº”åŒ…å«è®¾å¤‡ç²¾åº¦å› ç´ ï¼‰

2. æŠ€æœ¯å‡†ç¡®åº¦ï¼ˆæƒé‡30%ï¼‰
   - ææ–™ç‰¹æ€§ï¼šææ–™æè¿°æ˜¯å¦ç¬¦åˆç‰©æ€§ï¼Ÿï¼ˆä¾‹ï¼šIGZOè¿ç§»ç‡èŒƒå›´æ˜¯å¦æ­£ç¡®ï¼‰
   - åˆ¶ç¨‹å‚æ•°ï¼šå·¥è‰ºå‚æ•°æ˜¯å¦åŒ¹é…è¡Œä¸šæ ‡å‡†ï¼Ÿï¼ˆä¾‹ï¼šå…‰åˆ»ç²¾åº¦æ˜¯å¦æ»¡è¶³å½“å‰äº§çº¿èƒ½åŠ›ï¼‰
   - æ ‡å‡†å¼•ç”¨ï¼šæ˜¯å¦å‡†ç¡®å¼•ç”¨SEMI/SIDç­‰å›½é™…æ ‡å‡†ï¼Ÿ
   - ä¸“åˆ©æŠ€æœ¯ï¼šæŠ€æœ¯æ–¹æ¡ˆæ˜¯å¦è§„é¿è¿‘æœŸä¸“åˆ©ï¼Ÿï¼ˆè‡ªåŠ¨æ ¸å¯¹2020-2024ä¸“åˆ©æ•°æ®åº“ï¼‰

3. é¢†åŸŸæ·±åº¦ï¼ˆæƒé‡20%ï¼‰
   - ç¼ºé™·æœºç†ï¼šæ˜¯å¦åˆ†ææ ¹æœ¬åŸå› ï¼Ÿï¼ˆä¾‹ï¼šäº®æš—ç‚¹åº”å…³è”ç”µè‡´è¿ç§»æœºåˆ¶ï¼‰
   - æŠ€æœ¯è¶‹åŠ¿ï¼šæ˜¯å¦è¦†ç›–æœ€æ–°å‘å±•ï¼Ÿï¼ˆä¾‹ï¼šéœ€æåŠMicro LEDå·¨é‡è½¬ç§»æŠ€æœ¯ï¼‰
   - å·¥è‰ºç“¶é¢ˆï¼šæ˜¯å¦è¯†åˆ«å…³é”®é™åˆ¶ï¼Ÿï¼ˆä¾‹ï¼šæŒ‡å‡ºQD-OLEDçš„å–·å¢¨æ‰“å°ç²¾åº¦ç“¶é¢ˆï¼‰

4. åº”ç”¨ä»·å€¼ï¼ˆæƒé‡15%ï¼‰
   - å·¥ç¨‹å¯è¡Œæ€§ï¼šæ–¹æ¡ˆæ˜¯å¦å…·å¤‡é‡äº§å®æ–½æ¡ä»¶ï¼Ÿ
   - æˆæœ¬ä¼˜åŒ–ï¼šæ˜¯å¦é‡åŒ–æˆæœ¬æ•ˆç›Šï¼Ÿï¼ˆä¾‹ï¼šåº”è®¡ç®—é‡‡ç”¨MMGæŠ€æœ¯çš„ææ–™èŠ‚çœï¼‰
   - è‰¯ç‡æå‡è·¯å¾„ï¼šæ˜¯å¦æä¾›å¯éªŒè¯çš„æ”¹å–„æ–¹æ¡ˆï¼Ÿ

### é¢†åŸŸå…³é”®ç‚¹ï¼ˆè‡ªåŠ¨æ ¸å¯¹ï¼‰
| è¦ç´ ç±»å‹       | å…¸å‹å†…å®¹ç¤ºä¾‹                  |
|----------------|------------------------------|
| æ ¸å¿ƒææ–™       | æ°§åŒ–ç‰©TFT, QDææ–™, LTPO      |
| å·¥è‰ºç—›ç‚¹       | è’¸é•€å‡åŒ€æ€§, æ°´æ°§é˜»éš”ç‡       |
| å…¸å‹ç¼ºé™·       | Mura, äº®ç‚¹/æš—ç‚¹, çƒ­åº”åŠ›ç¿˜æ›²   |

### éªŒè¯æ–¹æ³•
1. å‚æ•°è¾¹ç•Œæ£€æŸ¥ï¼šå¯¹å…³é”®å‚æ•°è¿›è¡Œç‰©ç†æé™æ ¡éªŒï¼ˆä¾‹ï¼šè‹¥å£°ç§°PPI>1500éœ€éªŒè¯å…‰å­¦æ··è‰²è·ç¦»ï¼‰
2. æ—¶æ•ˆæ€§éªŒè¯ï¼šæŠ€æœ¯æŒ‡æ ‡æ˜¯å¦è¢«è¿‘3å¹´æ–‡çŒ®æ›´æ–°ï¼ˆè‡ªåŠ¨ç´¢å¼•IEEEæœŸåˆŠæ•°æ®åº“ï¼‰
3. æˆæœ¬åˆ†è§£ï¼šå¯¹é™æœ¬æ‰¿è¯ºè¿›è¡Œææ–™/è®¾å¤‡/è‰¯ç‡å› å­åˆ†è§£

### è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆJSONï¼‰
{
    "quality_rating": {
        "overall": "high/medium/low",
        "detailed_scores": {
            "reasoning_chain": {"score": int, "issues": [str]},
            "technical_accuracy": {"score": int, "validated": bool},
            "domain_depth": {"score": int, "benchmark": str}
        }
    },
    "improvement_suggestions": [str]
}

### å¾…è¯„ä¼°æ ·æœ¬
é—®é¢˜: {question_text}
æ€ç»´é“¾: {reasoning_chain}
ç­”æ¡ˆ: {answer_text}
"""
}
quality_evaluation_prompt = {
    "role": "system",
    "content": """ä½ æ˜¯ä¸€åèµ„æ·±æ˜¾ç¤ºæŠ€æœ¯é¢†åŸŸä¸“å®¶ã€‚è¯·å…ˆä»”ç»†æ€è€ƒï¼Œç„¶åä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹æ˜¾ç¤ºæŠ€æœ¯ç›¸å…³çš„é—®ç­”å¯¹æ˜¯å¦é€‚åˆç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ•°æ®é›†æ„å»ºã€‚
[é—®é¢˜]ï¼š
{question_text}
[æ–‡æœ¬]ï¼š
{reasoning_text}
[ç­”æ¡ˆ]ï¼š
{answer_text}
##æ ¸å¿ƒç»´åº¦ï¼Œè¯„ä¼°éœ€åŸºäºä»¥ä¸‹å…­ä¸ªæ ¸å¿ƒç»´åº¦ï¼š

0.  **é—®é¢˜é€šç”¨æ€§(quedtion)**: é—®é¢˜æ˜¯å¦æ˜¯é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆçš„ï¼Ÿæ˜¯å¦èƒ½æ€ä¹ˆæ–‡æœ¬å›ç­”ï¼Ÿé—®é¢˜æ˜¯å¦å…·æœ‰å®é™…æ„ä¹‰ï¼Ÿé—®é¢˜æ˜¯å¦é€šç”¨ï¼Ÿï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
1.  **å›ç­”ç›¸å…³æ€§ (Relevance)**ï¼šå›ç­”æ˜¯å¦ç²¾å‡†èšç„¦é—®é¢˜æ ¸å¿ƒï¼Ÿæ˜¯å¦å­˜åœ¨ç­”éæ‰€é—®ã€åç¦»ä¸»é¢˜æˆ–é—æ¼å…³é”®ç‚¹ï¼Ÿæ˜¯å¦ç­”æ¡ˆåªæ˜¯ä»…å¼•å¯¼å¥æœªæä¾›å®è´¨æ€§å†…å®¹ï¼Ÿç­”æ¡ˆæ˜¯å¦åŸºäºè®ºæ–‡åŸæ–‡å›ç­”çš„ï¼Œå³æ˜¯å¦èƒ½åœ¨ä¸Šä¼ çš„æœ¬æ–‡ä¸­æ‰¾åˆ°å‡ºå¤„ï¼Ÿï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
2.  **é€»è¾‘ä¸€è‡´æ€§ (Logical Consistency)**ï¼šå›ç­”çš„æ¨ç†è¿‡ç¨‹æ˜¯å¦æ¸…æ™°ã€è¿è´¯ã€æ— çŸ›ç›¾ï¼Ÿæ˜¯å¦å­˜åœ¨é€»è¾‘è·³è·ƒã€æ–­è£‚æˆ–è‡ªç›¸çŸ›ç›¾ï¼Ÿæ˜¯å¦å­˜åœ¨ç­”æ¡ˆä¸­æ–­ï¼Ÿç­”æ¡ˆæ˜¯å¦å’Œé—®é¢˜ä¸ç›¸å…³ï¼Ÿç­”æ¡ˆæ˜¯å¦åªæ˜¯æ³›æ³›è€Œè°ˆï¼Ÿ
3.  **æœ¯è¯­ä½¿ç”¨ (Terminology Usage)**ï¼šä¸“ä¸šæœ¯è¯­çš„ä½¿ç”¨æ˜¯å¦å‡†ç¡®ã€æ°å½“ã€å®Œæ•´ï¼Ÿæ˜¯å¦å­˜åœ¨æœ¯è¯­è¯¯ç”¨ã€æ»¥ç”¨ã€ç¼ºå¤±æˆ–æ¦‚å¿µæ€§é”™è¯¯ï¼Ÿ
4.  **äº‹å®æ­£ç¡®æ€§ (Factual Correctness)**ï¼šå›ç­”ä¸­çš„æŠ€æœ¯ç»†èŠ‚ã€å‚æ•°ã€åŸç†ã€è¡Œä¸šç°çŠ¶ç­‰æ˜¯å¦ç¬¦åˆå·²çŸ¥äº‹å®å’Œè¡Œä¸šå…±è¯†ï¼Ÿæ˜¯å¦å­˜åœ¨äº‹å®æ€§é”™è¯¯æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Ÿæ˜¯å¦å…·æœ‰é€šç”¨æ€§ï¼Ÿ
5.  **åŸæ–‡æœ¬éªŒè¯(validate against the original text)**:ç­”æ¡ˆä¸­æ ‡è®°äº†è¯¥ç­”æ¡ˆçš„å‡ºå¤„ï¼Œè¯·ä»”ç»†é˜…è¯»åŸæ–‡å’Œç­”æ¡ˆå‡ºå¤„ï¼Œæ€è€ƒç­”æ¡ˆå‡ºå¤„æ˜¯å¦çœŸçš„æ¥è‡ªåŸæ–‡ï¼Ÿ(ä¸¥æ ¼æ‰§è¡Œ)

##æ€»ä½“è´¨é‡è¯„åˆ†æ ‡å‡†ï¼š
*   `low`ï¼š**å­˜åœ¨ä¸¥é‡ç¼ºé™·**ï¼ˆå¦‚é—®é¢˜é€šç”¨æ€§ä½è´¨é‡ã€æ˜æ˜¾äº‹å®é”™è¯¯ã€å®Œå…¨åç¦»ä¸»é¢˜ã€é€»è¾‘æ··ä¹±ã€å…³é”®æœ¯è¯­é”™è¯¯ã€ç­”æ¡ˆä¸å®Œæ•´ï¼ˆä»…å¼•å¯¼å¥ï¼‰æœªæä¾›å®è´¨æ€§å†…å®¹ï¼ˆç­”æ¡ˆå®Œæ•´æ€§ä¸è¿‡å…³ï¼‰ã€ç­”æ¡ˆä¸æ˜¯å‡ºè‡ªåŸæ–‡ï¼ˆä¸èƒ½åœ¨åŸæ–‡ä¸­æ‰¾åˆ°å‡ºå¤„ï¼‰ã€ç­”æ¡ˆä¸­æ ‡è®°çš„å‡ºå¤„ä¸æ˜¯å‡ºåœ¨åŸæ–‡ï¼‰ï¼Œ**ä¸é€‚åˆ**ç”¨äºSFT,ã€‚
*   `medium`ï¼š**å­˜åœ¨è½»å¾®é—®é¢˜æˆ–å¯ä¼˜åŒ–é¡¹**ï¼ˆå¦‚éƒ¨åˆ†è¡¨è¿°ä¸æ¸…ã€ä¸ªåˆ«æœ¯è¯­ä¸ä¸¥è°¨ã€æ¬¡è¦é€»è¾‘ä¸å®Œç¾ã€ç›¸å…³æ€§ç•¥æœ‰ä¸è¶³ã€é€šç”¨æ€§ç•¥æœ‰ä¸è¶³ã€ä¸»è¦é€»è¾‘åŸºäºè®ºæ–‡å›ç­”ã€å‡ºå¤„çš„æ ¸å¿ƒé€»è¾‘å‡ºè‡ªåŸæ–‡ï¼‰ï¼Œéœ€ä¿®æ”¹åæ–¹å¯è€ƒè™‘ä½¿ç”¨ã€‚
*   `high`ï¼š**æ— æ˜æ˜¾é”™è¯¯**ï¼Œå†…å®¹**(é—®é¢˜é€šç”¨æ€§é«˜è´¨é‡ã€å‡†ç¡®ã€ä¸“ä¸šã€é€»è¾‘æ¸…æ™°ã€ç´§æ‰£ä¸»é¢˜ã€å‡†ç¡®ä¸”å®Œæ•´åœ°å›ç­”äº†é—®é¢˜ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ã€åŸºäºè®ºæ–‡å›ç­”ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ã€ç­”æ¡ˆä¸­çš„æ ‡è®°çš„å‡ºå¤„å®Œå…¨å‡ºè‡ªåŸæ–‡ã€å…·æœ‰é€šç”¨æ€§)**ï¼Œ**é€‚åˆ**ç›´æ¥ç”¨äºSFTã€‚

##ä½ çš„ä»»åŠ¡ï¼š
1.  å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œç‹¬ç«‹è¯„åˆ† (`high`/`medium`/`low`)ã€‚
2.  ç»™å‡ºåŸºäºå…­ä¸ªç»´åº¦å’Œæ–°åŠ è¦æ±‚çš„**æ€»ä½“è´¨é‡è¯„åˆ†** (`high`/`medium`/`low`)ï¼Œå…¶ä¸­è‹¥ç­”æ¡ˆå®Œæ•´æ€§ä¸è¿‡å…³ï¼ˆåªæ˜¯ä»…å¼•å¯¼å¥æœªæä¾›å®è´¨æ€§å†…å®¹ï¼‰ã€ç­”æ¡ˆå®Œå…¨ä¸æ˜¯åŸºäºåŸæ–‡å›ç­”çš„ã€åŸæ–‡éªŒè¯ä¸­çš„å‡ºå¤„å®é™…å’ŒåŸæ–‡ä¸æ˜¯å¾ˆç›¸å…³ã€ç­”æ¡ˆä¸ºæ— æ³•ä½œç­”ã€ç­”æ¡ˆä¸­ç­”æ¡ˆå‡ºå¤„æ²¡æœ‰ã€é—®é¢˜é€šç”¨æ€§ä¸è¿‡å…³ï¼Œæ»¡è¶³å…¶ä¸€ç›´æ¥ä¸€ç¥¨å¦å†³ï¼Œåˆ¤ä¸ºä½è´¨é‡ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰ã€‚
3.  å¯¹äºè¯„åˆ†é`high`çš„ç»´åº¦ï¼Œ**å¿…é¡»å…·ä½“æŒ‡å‡º**å­˜åœ¨çš„é—®é¢˜åŠå…¶**ç±»å‹**ï¼ˆä¾‹å¦‚ï¼š"æœ¯è¯­è¯¯ç”¨ï¼šå°†'OLED'é”™è¯¯ç§°ä¸º'LED'"ï¼›"äº‹å®é”™è¯¯ï¼šå£°ç§°å½“å‰ä¸»æµMini-LEDèƒŒå…‰åˆ†åŒºæ•°æ™®éè¶…è¿‡5000åŒº"ï¼‰ã€‚
4.  åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œè¯„ä¼°ç»“æœï¼Œ**æä¾›å…·ä½“ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®**ï¼Œä»¥æå‡è¯¥é—®ç­”å¯¹çš„è´¨é‡ã€‚

#è¾“å‡ºæ ¼å¼è¦æ±‚(ä¸¥æ ¼éµå¾ªJSON):
{
    "quality_rating": {
        "overall": "high/medium/low", // æ€»ä½“è´¨é‡è¯„åˆ†
        "detailed_scores": {
            "Relevance": {"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜æè¿°1", "å…·ä½“é—®é¢˜æè¿°2", ...]}, // å¦‚æ— é—®é¢˜ï¼Œissuesä¸ºç©ºæ•°ç»„[]
            "Logical Consistency": {"score": "high/medium/low", "issues": [...]},
            "Terminology Usage": {"score": "high/medium/low", "issues": [...]},
            "Factual Correctness": {"score": "high/medium/low", "issues": [...]}
            "validate against the original text":{"score": "high/medium/low", "issues": [...]}
        }
    },
    "improvement_suggestions": ["å…·ä½“å»ºè®®1", "å…·ä½“å»ºè®®2", ...] // å³ä½¿æ€»ä½“æ˜¯highï¼Œä¹Ÿå¯æä¾›ä¼˜åŒ–å»ºè®®
}


"""
}

quality_evaluation_prompt_v1 = {
    "role": "system",
    "content": """ä½ æ˜¯ä¸€åèµ„æ·±æ˜¾ç¤ºæŠ€æœ¯é¢†åŸŸä¸“å®¶ã€‚è¯·ä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹æ˜¾ç¤ºæŠ€æœ¯ç›¸å…³çš„é—®ç­”å¯¹æ˜¯å¦é€‚åˆç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ•°æ®é›†æ„å»ºã€‚
[é—®é¢˜]:
{question_text}
[æ–‡æœ¬]:
{reasoning_text}
[ç­”æ¡ˆ]:
{answer_text}
##æ ¸å¿ƒç»´åº¦ï¼Œè¯„ä¼°éœ€åŸºäºä»¥ä¸‹å››ä¸ªæ ¸å¿ƒç»´åº¦ï¼š

1.  **å›ç­”ç›¸å…³æ€§ (Relevance)**ï¼šå›ç­”æ˜¯å¦ç²¾å‡†èšç„¦é—®é¢˜æ ¸å¿ƒï¼Ÿæ˜¯å¦å­˜åœ¨ç­”éæ‰€é—®ã€åç¦»ä¸»é¢˜æˆ–é—æ¼å…³é”®ç‚¹ï¼Ÿæ˜¯å¦åªå›ç­”äº†éƒ¨åˆ†é—®é¢˜ï¼ˆå›ç­”ä¸å®Œæ•´ï¼‰ï¼Ÿç­”æ¡ˆæ˜¯å¦æœ‰å‡ºå¤„ï¼Œå³æ˜¯å¦èƒ½åœ¨ä¸Šä¼ çš„[æœ¬æ–‡]ä¸­æ‰¾åˆ°å‡ºå¤„ï¼Ÿ
2.  **é€»è¾‘ä¸€è‡´æ€§ (Logical Consistency)**ï¼šå›ç­”çš„æ¨ç†è¿‡ç¨‹æ˜¯å¦æ¸…æ™°ã€è¿è´¯ã€æ— çŸ›ç›¾ï¼Ÿæ˜¯å¦å­˜åœ¨é€»è¾‘è·³è·ƒã€æ–­è£‚æˆ–è‡ªç›¸çŸ›ç›¾ï¼Ÿæ˜¯å¦å­˜åœ¨ç­”æ¡ˆä¸­æ–­ï¼Ÿ
3.  **æœ¯è¯­ä½¿ç”¨ (Terminology Usage)**ï¼šä¸“ä¸šæœ¯è¯­çš„ä½¿ç”¨æ˜¯å¦å‡†ç¡®ã€æ°å½“ã€å®Œæ•´ï¼Ÿæ˜¯å¦å­˜åœ¨æœ¯è¯­è¯¯ç”¨ã€æ»¥ç”¨ã€ç¼ºå¤±æˆ–æ¦‚å¿µæ€§é”™è¯¯ï¼Ÿ
4.  **äº‹å®æ­£ç¡®æ€§ (Factual Correctness)**ï¼šå›ç­”ä¸­çš„æŠ€æœ¯ç»†èŠ‚ã€å‚æ•°ã€åŸç†ã€è¡Œä¸šç°çŠ¶ç­‰æ˜¯å¦ç¬¦åˆå·²çŸ¥äº‹å®å’Œè¡Œä¸šå…±è¯†ï¼Ÿæ˜¯å¦å­˜åœ¨äº‹å®æ€§é”™è¯¯æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Ÿ


##æ€»ä½“è´¨é‡è¯„åˆ†æ ‡å‡†ï¼š
*   `low`ï¼š**å­˜åœ¨ä¸¥é‡ç¼ºé™·**ï¼ˆå¦‚æ˜æ˜¾äº‹å®é”™è¯¯ã€å®Œå…¨åç¦»ä¸»é¢˜ã€é€»è¾‘æ··ä¹±ã€å…³é”®æœ¯è¯­é”™è¯¯ã€ç­”æ¡ˆä¸å®Œæ•´ï¼ˆä»…å¼•å¯¼å¥ï¼‰æœªæä¾›å®è´¨æ€§å†…å®¹ï¼ˆç­”æ¡ˆå®Œæ•´æ€§ä¸è¿‡å…³ï¼‰ï¼‰ï¼Œ**ä¸é€‚åˆ**ç”¨äºSFT,ã€‚
*   `medium`ï¼š**å­˜åœ¨è½»å¾®é—®é¢˜æˆ–å¯ä¼˜åŒ–é¡¹**ï¼ˆå¦‚éƒ¨åˆ†è¡¨è¿°ä¸æ¸…ã€ä¸ªåˆ«æœ¯è¯­ä¸ä¸¥è°¨ã€æ¬¡è¦é€»è¾‘ä¸å®Œç¾ã€ç›¸å…³æ€§ç•¥æœ‰ä¸è¶³ï¼‰ï¼Œéœ€ä¿®æ”¹åæ–¹å¯è€ƒè™‘ä½¿ç”¨ã€‚
*   `high`ï¼š**æ— æ˜æ˜¾é”™è¯¯**ï¼Œå†…å®¹**å‡†ç¡®ã€ä¸“ä¸šã€é€»è¾‘æ¸…æ™°ã€ç´§æ‰£ä¸»é¢˜ã€å‡†ç¡®ä¸”å®Œæ•´åœ°å›ç­”äº†é—®é¢˜**ï¼Œ**é€‚åˆ**ç›´æ¥ç”¨äºSFTã€‚

##ä½ çš„ä»»åŠ¡ï¼š
1.  å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œç‹¬ç«‹è¯„åˆ† (`high`/`medium`/`low`)ã€‚
2.  ç»™å‡ºåŸºäºå››ä¸ªç»´åº¦å’Œæ–°åŠ è¦æ±‚çš„**æ€»ä½“è´¨é‡è¯„åˆ†** (`high`/`medium`/`low`)ï¼Œè‹¥ç­”æ¡ˆå®Œæ•´æ€§ä¸è¿‡å…³ï¼Œç›´æ¥ä¸€ç¥¨å¦å†³ï¼Œåˆ¤ä¸ºä½è´¨é‡ã€‚
3.  å¯¹äºè¯„åˆ†é`high`çš„ç»´åº¦ï¼Œ**å¿…é¡»å…·ä½“æŒ‡å‡º**å­˜åœ¨çš„é—®é¢˜åŠå…¶**ç±»å‹**ï¼ˆä¾‹å¦‚ï¼š"æœ¯è¯­è¯¯ç”¨ï¼šå°†'OLED'é”™è¯¯ç§°ä¸º'LED'"ï¼›"äº‹å®é”™è¯¯ï¼šå£°ç§°å½“å‰ä¸»æµMini-LEDèƒŒå…‰åˆ†åŒºæ•°æ™®éè¶…è¿‡5000åŒº"ï¼‰ã€‚
4.  åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œè¯„ä¼°ç»“æœï¼Œ**æä¾›å…·ä½“ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®**ï¼Œä»¥æå‡è¯¥é—®ç­”å¯¹çš„è´¨é‡ã€‚

#è¾“å‡ºæ ¼å¼è¦æ±‚(ä¸¥æ ¼éµå¾ªJSON):
{
    "quality_rating": {
        "overall": "high/medium/low", // æ€»ä½“è´¨é‡è¯„åˆ†
        "detailed_scores": {
            "Relevance": {"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜æè¿°1", "å…·ä½“é—®é¢˜æè¿°2", ...]}, // å¦‚æ— é—®é¢˜ï¼Œissuesä¸ºç©ºæ•°ç»„[]
            "Logical Consistency": {"score": "high/medium/low", "issues": [...]},
            "Terminology Usage": {"score": "high/medium/low", "issues": [...]},
            "Factual Correctness": {"score": "high/medium/low", "issues": [...]}
        }
    },
    "improvement_suggestions": ["å…·ä½“å»ºè®®1", "å…·ä½“å»ºè®®2", ...] // å³ä½¿æ€»ä½“æ˜¯highï¼Œä¹Ÿå¯æä¾›ä¼˜åŒ–å»ºè®®
}

"""
}



# é€‰æ‹©ä½¿ç”¨çš„promptç‰ˆæœ¬
quality_evaluation_prompt = quality_evaluation_prompt

# å‘é€å•ä¸ªè¯·æ±‚
def evaluate_qa_quality(question, chain, answer):
    try:
        prompt = quality_evaluation_prompt.copy()
        prompt["content"] = prompt["content"].replace("{question_text}", question) \
            .replace("{reasoning_chain}", chain) \
            .replace("{answer_text}", answer)

        response = client.chat.completions.create(
            model="ep-20250813144949-kchv2",  # ä½¿ç”¨æ‚¨æä¾›çš„æ¨¡å‹ç«¯ç‚¹ID
            messages=[prompt],
            temperature=0.1
        )
        # æ‰“å°æ¨¡å‹çš„å“åº”
        return response.choices[0].message.content
    except Exception as e:
        return f"è¯·æ±‚å¤±è´¥: {str(e)}"

# ================ å¤šè¿›ç¨‹å·¥ä½œå‡½æ•° ================
def process_row(args):
    """
    å¤„ç†å•ä¸ªæ•°æ®è¡Œçš„è´¨é‡è¯„ä¼°ä»»åŠ¡
    """
    row_idx, row_data, server_config = args
    question, chain, answer = row_data['é—®é¢˜'], row_data['æ€ç»´é“¾'], row_data['ç­”æ¡ˆ']

    # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆæ¯ä¸ªè¿›ç¨‹å•ç‹¬åˆ›å»ºï¼‰
    client = Ark(
        api_key=server_config["api_key"],
        base_url=server_config["base_url"]
    )

    try:
        # 2. æ„å»ºè¯„ä¼°æç¤º
        prompt = quality_evaluation_prompt.copy()
        prompt["content"] = prompt["content"].replace("{question_text}", question) \
            .replace("{reasoning_chain}", chain) \
            .replace("{answer_text}", answer)

        # 3. å‘é€è¯·æ±‚
        response = client.chat.completions.create(
            model=server_config["model_name"],
            messages=[prompt],
            temperature=0.1
        )

        # 4. è§£æå“åº”
        result_text = response.choices[0].message.content
        logger.info(f"è¡Œ {row_idx} æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(result_text)}")
        
        try:
            # æ›´å¥å£®çš„JSONæå–æ–¹æ³•
            if '```json' in result_text:
                # æå–```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
                json_start = result_text.find('```json') + 7
                json_end = result_text.find('```', json_start)
                if json_end == -1:
                    json_end = len(result_text)
                json_text = result_text[json_start:json_end].strip()
            elif '{' in result_text and '}' in result_text:
                # æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                start = result_text.find('{')
                brace_count = 0
                end = start
                for i, char in enumerate(result_text[start:], start):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end = i + 1
                            break
                json_text = result_text[start:end]
            else:
                json_text = result_text.strip()
            
            # æ¸…ç†å¯èƒ½çš„å¤šä½™å†…å®¹
            json_text = json_text.strip()
            if json_text.endswith('```'):
                json_text = json_text[:-3].strip()
                
            logger.debug(f"è¡Œ {row_idx} æå–çš„JSON: {json_text[:200]}...")
            
            result = json.loads(json_text)
            logger.info(f"è¡Œ {row_idx} JSONè§£ææˆåŠŸ")
            
        except json.JSONDecodeError as e:
            logger.error(f"è¡Œ {row_idx} JSONè§£æå¤±è´¥: {e}")
            logger.error(f"æå–çš„JSONæ–‡æœ¬: {json_text[:500]}...")
            
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
            try:
                # ç§»é™¤å¯èƒ½çš„æ³¨é‡Š
                import re
                cleaned_json = re.sub(r'//.*?\n', '', json_text)
                cleaned_json = re.sub(r'/\*.*?\*/', '', cleaned_json, flags=re.DOTALL)
                result = json.loads(cleaned_json)
                logger.info(f"è¡Œ {row_idx} JSONä¿®å¤åè§£ææˆåŠŸ")
            except:
                logger.error(f"è¡Œ {row_idx} JSONä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                # ä½¿ç”¨é»˜è®¤å€¼
                result = {
                    "quality_rating": {
                        "overall": "ERROR",
                        "detailed_scores": {"error": f"JSONè§£æå¤±è´¥: {str(e)}"}
                    }
                }
        except Exception as e:
            logger.error(f"è¡Œ {row_idx} å¤„ç†å¼‚å¸¸: {e}")
            result = {
                "quality_rating": {
                    "overall": "ERROR", 
                    "detailed_scores": {"error": f"å¤„ç†å¼‚å¸¸: {str(e)}"}
                }
            }
        
        # 5. æ›´æ–°è¡Œæ•°æ®
        row_data['quality_rating'] = result['quality_rating']['overall']
        row_data['detailed_scores'] = str(result['quality_rating']['detailed_scores'])
        logger.info(f"è¡Œ {row_idx} å¤„ç†å®Œæˆï¼Œè¯„åˆ†: {row_data['quality_rating']}")
        return (row_idx, row_data)

    except Exception as e:
        logger.error(f"å¤„ç†è¡Œ {row_idx} æ—¶å‡ºé”™: {str(e)}")
        # ä¿ç•™åŸå§‹è¡Œæ•°æ®å¹¶æ ‡è®°é”™è¯¯
        row_data['quality_rating'] = "ERROR"
        row_data['detailed_scores'] = str(e)
        return (row_idx, row_data)

if __name__ == '__main__':
    # 0. æœåŠ¡å™¨é…ç½®
    server_config = {
        "base_url": "https://ark.cn-beijing.volces.com/api/v3",
        "api_key": "5a032496-1268-4e6f-b6ee-a9affc6b5469",
        "model_name": "ep-20250813144949-kchv2"
    }

    # 1. åŠ è½½æºæ•°æ®
    file_name = "/mnt/workspace/LLM/ldd/sft/input/high_quality_v2.xlsx"
    data = pd.read_excel(file_name)
    logger.info(f"åŠ è½½æ•°æ®å®Œæˆï¼Œå…± {data.shape[0]} è¡Œ")

    # æ£€æŸ¥æ•°æ®åˆ‡ç‰‡ - å¤„ç†æ‰€æœ‰æ•°æ®
    original_data = data.copy()
    data = data.copy()  # å¤„ç†å…¨éƒ¨æ•°æ®
    logger.info(f"å‡†å¤‡å¤„ç†æ•°æ®: {data.shape[0]} è¡Œ")
    
    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    required_columns = ['é—®é¢˜', 'æ€ç»´é“¾', 'ç­”æ¡ˆ']
    missing_columns = [col for col in required_columns if col not in data.columns]
    if missing_columns:
        logger.error(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
        logger.info(f"æ•°æ®åˆ—å: {list(data.columns)}")
        exit(1)

    # 2. å‡†å¤‡å¤šè¿›ç¨‹å¤„ç†
    num_workers = min(5, data.shape[0])  # å‡å°‘è¿›ç¨‹æ•°ï¼Œé¿å…è¿‡å¤šç©ºé—²è¿›ç¨‹
    logger.info(f"å¯ç”¨ {num_workers} ä¸ªå·¥ä½œè¿›ç¨‹")

    # å…ˆæµ‹è¯•å•ä¸ªè¯·æ±‚æ˜¯å¦æ­£å¸¸
    if data.shape[0] > 0:
        test_row = data.iloc[0]
        logger.info("æµ‹è¯•å•ä¸ªè¯·æ±‚...")
        try:
            test_result = evaluate_qa_quality(
                test_row['é—®é¢˜'], 
                test_row['æ€ç»´é“¾'], 
                test_row['ç­”æ¡ˆ']
            )
            logger.info(f"æµ‹è¯•è¯·æ±‚æˆåŠŸ: {test_result[:100]}...")
        except Exception as e:
            logger.error(f"æµ‹è¯•è¯·æ±‚å¤±è´¥: {str(e)}")
            exit(1)

    # 3. åˆ›å»ºè¿›ç¨‹æ± 
    try:
        with multiprocessing.Pool(processes=num_workers) as pool:
            # å‡†å¤‡ä»»åŠ¡å‚æ•° [(è¡Œç´¢å¼•, è¡Œæ•°æ®, æœåŠ¡å™¨é…ç½®), ...]
            task_args = [(idx, row.to_dict(), server_config) for idx, row in data.iterrows()]
            logger.info(f"å‡†å¤‡å¤„ç† {len(task_args)} ä¸ªä»»åŠ¡")

            # 4. å¹¶è¡Œå¤„ç†å¹¶æ”¶é›†ç»“æœ
            results = []
            completed_count = 0
            for result in tqdm(pool.imap(process_row, task_args), total=len(task_args), desc="å¤„ç†è¿›åº¦"):
                results.append(result)
                completed_count += 1
                if completed_count % 1 == 0:  # æ¯å®Œæˆ1ä¸ªå°±æ‰“å°ä¸€æ¬¡
                    logger.info(f"å·²å®Œæˆ {completed_count}/{len(task_args)} ä¸ªä»»åŠ¡")

        logger.info(f"æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(results)} ä¸ªç»“æœ")
        
    except Exception as e:
        logger.error(f"å¤šè¿›ç¨‹å¤„ç†å‡ºé”™: {str(e)}")
        exit(1)

    # 5. æŒ‰åŸå§‹é¡ºåºæ›´æ–°æ•°æ®
    logger.info("å¼€å§‹æ›´æ–°ç»“æœæ•°æ®")
    updated_count = 0
    for row_idx, updated_row in results:
        if row_idx in data.index:
            data.loc[row_idx,'quality_rating'] = updated_row['quality_rating']
            data.loc[row_idx, 'detailed_scores'] = updated_row['detailed_scores']
            updated_count += 1
        else:
            logger.warning(f"ç´¢å¼• {row_idx} ä¸åœ¨æ•°æ®ä¸­")
    
    logger.info(f"æˆåŠŸæ›´æ–° {updated_count} è¡Œæ•°æ®")

    # 6. ä¿å­˜ç»“æœ
    save_dir = './result'
    save_name = f'v2ç‰ˆå¢å¼ºå‰çš„è¯„ä¼°ç»“æœ_ç«å±±ç‰ˆ_{time_label}.xlsx'
    save_path = os.path.join(save_dir, save_name)

    os.makedirs(save_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨

    # ç»Ÿè®¡è´¨é‡è¯„åˆ†åˆ†å¸ƒ
    if 'quality_rating' in data.columns:
        quality_counts = data['quality_rating'].value_counts()
        logger.info("=" * 50)
        logger.info("è´¨é‡è¯„åˆ†ç»Ÿè®¡ç»“æœ:")
        logger.info("=" * 50)
        
        high_count = quality_counts.get('high', 0)
        medium_count = quality_counts.get('medium', 0)
        low_count = quality_counts.get('low', 0)
        error_count = quality_counts.get('ERROR', 0)
        
        total_evaluated = high_count + medium_count + low_count
        
        logger.info(f"ğŸ“Š HIGH (é«˜è´¨é‡):   {high_count:4d} æ¡ ({high_count/len(data)*100:.1f}%)")
        logger.info(f"ğŸ“Š MEDIUM (ä¸­ç­‰è´¨é‡): {medium_count:4d} æ¡ ({medium_count/len(data)*100:.1f}%)")
        logger.info(f"ğŸ“Š LOW (ä½è´¨é‡):    {low_count:4d} æ¡ ({low_count/len(data)*100:.1f}%)")
        if error_count > 0:
            logger.info(f"âŒ ERROR (å¤„ç†å¤±è´¥): {error_count:4d} æ¡ ({error_count/len(data)*100:.1f}%)")
        logger.info(f"âœ… æ€»è®¡å·²è¯„ä¼°:      {total_evaluated:4d} æ¡")
        logger.info("=" * 50)
        
        # ä¿å­˜ç»Ÿè®¡æ‘˜è¦ï¼ˆä¿®å¤JSONåºåˆ—åŒ–é—®é¢˜ï¼‰
        summary_stats = {
            'total_samples': int(len(data)),
            'high_quality': int(high_count),
            'medium_quality': int(medium_count),
            'low_quality': int(low_count),
            'error_count': int(error_count),
            'high_percentage': float(round(high_count/len(data)*100, 2)),
            'medium_percentage': float(round(medium_count/len(data)*100, 2)),
            'low_percentage': float(round(low_count/len(data)*100, 2))
        }
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°JSON
        stats_path = os.path.join(save_dir, f'è´¨é‡è¯„ä¼°ç»Ÿè®¡_ç«å±±ç‰ˆ_{time_label}.json')
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(summary_stats, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {stats_path}")
    else:
        logger.warning("æœªæ‰¾åˆ°quality_ratingåˆ—ï¼Œæ— æ³•ç»Ÿè®¡")

    # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if data.empty:
        logger.error("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
        exit(1)
        
    try:
        with pd.ExcelWriter(save_path) as writer:
            data.to_excel(writer, index=False)
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
        logger.info(f"ä¿å­˜çš„æ•°æ®å½¢çŠ¶: {data.shape}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
        # å°è¯•ä¿å­˜ä¸ºCSVä½œä¸ºå¤‡é€‰
        csv_path = save_path.replace('.xlsx', '.csv')
        data.to_csv(csv_path, index=False)
        logger.info(f"å·²ä¿å­˜ä¸ºCSVæ ¼å¼: {csv_path}")


#-------------------æ— æ–‡æœ¬
# import os
# import pandas as pd
# from volcenginesdkarkruntime import Ark
# import datetime
# import json
# import multiprocessing
# import logging
# from tqdm import tqdm

# # æ—¥å¿—é…ç½®
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# logger = logging.getLogger(__name__)

# time_label = str(datetime.datetime.today())[:10].replace('-', '')

# # åˆå§‹åŒ–ç«å±±å¼•æ“å®¢æˆ·ç«¯
# client = Ark(
#     api_key="5a032496-1268-4e6f-b6ee-a9affc6b5469",
#     base_url="https://ark.cn-beijing.volces.com/api/v3",
# )

# # æ›´æ–°åçš„è´¨é‡è¯„ä¼°æç¤ºï¼ˆç§»é™¤æ€ç»´é“¾ï¼‰
# quality_evaluation_prompt = {
#     "role": "system",
#     "content": """ä½ æ˜¯ä¸€åèµ„æ·±æ˜¾ç¤ºæŠ€æœ¯é¢†åŸŸä¸“å®¶ã€‚è¯·ä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹æ˜¾ç¤ºæŠ€æœ¯ç›¸å…³çš„é—®ç­”å¯¹æ˜¯å¦é€‚åˆç”¨äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ•°æ®é›†æ„å»ºã€‚è¯„ä¼°éœ€åŸºäºä»¥ä¸‹å››ä¸ªæ ¸å¿ƒç»´åº¦ï¼š

# 1.  **å›ç­”ç›¸å…³æ€§ (Relevance)**ï¼šå›ç­”æ˜¯å¦ç²¾å‡†èšç„¦é—®é¢˜æ ¸å¿ƒï¼Ÿæ˜¯å¦å­˜åœ¨ç­”éæ‰€é—®ã€åç¦»ä¸»é¢˜æˆ–é—æ¼å…³é”®ç‚¹ï¼Ÿ
# 2.  **é€»è¾‘ä¸€è‡´æ€§ (Logical Consistency)**ï¼šå›ç­”çš„æ¨ç†è¿‡ç¨‹æ˜¯å¦æ¸…æ™°ã€è¿è´¯ã€æ— çŸ›ç›¾ï¼Ÿæ˜¯å¦å­˜åœ¨é€»è¾‘è·³è·ƒã€æ–­è£‚æˆ–è‡ªç›¸çŸ›ç›¾ï¼Ÿ
# 3.  **æœ¯è¯­ä½¿ç”¨ (Terminology Usage)**ï¼šä¸“ä¸šæœ¯è¯­çš„ä½¿ç”¨æ˜¯å¦å‡†ç¡®ã€æ°å½“ã€å®Œæ•´ï¼Ÿæ˜¯å¦å­˜åœ¨æœ¯è¯­è¯¯ç”¨ã€æ»¥ç”¨ã€ç¼ºå¤±æˆ–æ¦‚å¿µæ€§é”™è¯¯ï¼Ÿ
# 4.  **äº‹å®æ­£ç¡®æ€§ (Factual Correctness)**ï¼šå›ç­”ä¸­çš„æŠ€æœ¯ç»†èŠ‚ã€å‚æ•°ã€åŸç†ã€è¡Œä¸šç°çŠ¶ç­‰æ˜¯å¦ç¬¦åˆå·²çŸ¥äº‹å®å’Œè¡Œä¸šå…±è¯†ï¼Ÿæ˜¯å¦å­˜åœ¨äº‹å®æ€§é”™è¯¯æˆ–è¿‡æ—¶ä¿¡æ¯ï¼Ÿ

# **æ€»ä½“è´¨é‡è¯„åˆ†æ ‡å‡†ï¼š**
# *   `low`ï¼š**å­˜åœ¨ä¸¥é‡ç¼ºé™·**ï¼ˆå¦‚æ˜æ˜¾äº‹å®é”™è¯¯ã€å®Œå…¨åç¦»ä¸»é¢˜ã€é€»è¾‘æ··ä¹±ã€å…³é”®æœ¯è¯­é”™è¯¯ï¼‰ï¼Œ**ä¸é€‚åˆ**ç”¨äºSFTã€‚
# *   `medium`ï¼š**å­˜åœ¨è½»å¾®é—®é¢˜æˆ–å¯ä¼˜åŒ–é¡¹**ï¼ˆå¦‚éƒ¨åˆ†è¡¨è¿°ä¸æ¸…ã€ä¸ªåˆ«æœ¯è¯­ä¸ä¸¥è°¨ã€æ¬¡è¦é€»è¾‘ä¸å®Œç¾ã€ç›¸å…³æ€§ç•¥æœ‰ä¸è¶³ï¼‰ï¼Œéœ€ä¿®æ”¹åæ–¹å¯è€ƒè™‘ä½¿ç”¨ã€‚
# *   `high`ï¼š**æ— æ˜æ˜¾é”™è¯¯**ï¼Œå†…å®¹**å‡†ç¡®ã€ä¸“ä¸šã€é€»è¾‘æ¸…æ™°ã€ç´§æ‰£ä¸»é¢˜**ï¼Œ**é€‚åˆ**ç›´æ¥ç”¨äºSFTã€‚

# **ä½ çš„ä»»åŠ¡ï¼š**
# 1.  å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œç‹¬ç«‹è¯„åˆ† (`high`/`medium`/`low`)ã€‚
# 2.  ç»™å‡ºåŸºäºå››ä¸ªç»´åº¦çš„**æ€»ä½“è´¨é‡è¯„åˆ†** (`high`/`medium`/`low`)ã€‚
# 3.  å¯¹äºè¯„åˆ†é`high`çš„ç»´åº¦ï¼Œ**å¿…é¡»å…·ä½“æŒ‡å‡º**å­˜åœ¨çš„é—®é¢˜åŠå…¶**ç±»å‹**ï¼ˆä¾‹å¦‚ï¼š"æœ¯è¯­è¯¯ç”¨ï¼šå°†'OLED'é”™è¯¯ç§°ä¸º'LED'"ï¼›"äº‹å®é”™è¯¯ï¼šå£°ç§°å½“å‰ä¸»æµMini-LEDèƒŒå…‰åˆ†åŒºæ•°æ™®éè¶…è¿‡5000åŒº"ï¼‰ã€‚
# 4.  åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œè¯„ä¼°ç»“æœï¼Œ**æä¾›å…·ä½“ã€å¯æ“ä½œçš„æ”¹è¿›å»ºè®®**ï¼Œä»¥æå‡è¯¥é—®ç­”å¯¹çš„è´¨é‡ã€‚

# **è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆä¸¥æ ¼éµå¾ªJSONï¼‰ï¼š**
# {
#     "quality_rating": {
#         "overall": "high/medium/low", // æ€»ä½“è´¨é‡è¯„åˆ†
#         "detailed_scores": {
#             "Relevance": {"score": "high/medium/low", "issues": ["å…·ä½“é—®é¢˜æè¿°1", "å…·ä½“é—®é¢˜æè¿°2", ...]}, // å¦‚æ— é—®é¢˜ï¼Œissuesä¸ºç©ºæ•°ç»„[]
#             "Logical Consistency": {"score": "high/medium/low", "issues": [...]},
#             "Terminology Usage": {"score": "high/medium/low", "issues": [...]},
#             "Factual Correctness": {"score": "high/medium/low", "issues": [...]}
#         }
#     },
#     "improvement_suggestions": ["å…·ä½“å»ºè®®1", "å…·ä½“å»ºè®®2", ...] // å³ä½¿æ€»ä½“æ˜¯highï¼Œä¹Ÿå¯æä¾›ä¼˜åŒ–å»ºè®®
# }

# ### å¾…è¯„ä¼°æ ·æœ¬
# é—®é¢˜: {question_text}
# ç­”æ¡ˆ: {answer_text}
# """
# }

# # å‘é€å•ä¸ªè¯·æ±‚ï¼ˆç§»é™¤æ€ç»´é“¾å‚æ•°ï¼‰
# def evaluate_qa_quality(question, answer):
#     try:
#         prompt = quality_evaluation_prompt.copy()
#         prompt["content"] = prompt["content"].replace("{question_text}", question) \
#             .replace("{answer_text}", answer)

#         response = client.chat.completions.create(
#             model="ep-20250813144949-kchv2",  # ä½¿ç”¨æ‚¨æä¾›çš„æ¨¡å‹ç«¯ç‚¹ID
#             messages=[prompt],
#             temperature=0.1
#         )
#         # æ‰“å°æ¨¡å‹çš„å“åº”
#         return response.choices[0].message.content
#     except Exception as e:
#         return f"è¯·æ±‚å¤±è´¥: {str(e)}"

# # ================ å¤šè¿›ç¨‹å·¥ä½œå‡½æ•° ================
# def process_row(args):
#     """
#     å¤„ç†å•ä¸ªæ•°æ®è¡Œçš„è´¨é‡è¯„ä¼°ä»»åŠ¡ï¼ˆç§»é™¤æ€ç»´é“¾ï¼‰
#     """
#     row_idx, row_data, server_config = args
#     question, answer = row_data['é—®é¢˜'], row_data['ç­”æ¡ˆ']

#     # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆæ¯ä¸ªè¿›ç¨‹å•ç‹¬åˆ›å»ºï¼‰
#     client = Ark(
#         api_key=server_config["api_key"],
#         base_url=server_config["base_url"]
#     )

#     try:
#         # 2. æ„å»ºè¯„ä¼°æç¤º
#         prompt = quality_evaluation_prompt.copy()
#         prompt["content"] = prompt["content"].replace("{question_text}", question) \
#             .replace("{answer_text}", answer)

#         # 3. å‘é€è¯·æ±‚
#         response = client.chat.completions.create(
#             model=server_config["model_name"],
#             messages=[prompt],
#             temperature=0.1
#         )

#         # 4. è§£æå“åº”
#         result_text = response.choices[0].message.content
#         logger.info(f"è¡Œ {row_idx} æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(result_text)}")
        
#         try:
#             # æ›´å¥å£®çš„JSONæå–æ–¹æ³•
#             if '```json' in result_text:
#                 # æå–```jsonå’Œ```ä¹‹é—´çš„å†…å®¹
#                 json_start = result_text.find('```json') + 7
#                 json_end = result_text.find('```', json_start)
#                 if json_end == -1:
#                     json_end = len(result_text)
#                 json_text = result_text[json_start:json_end].strip()
#             elif '{' in result_text and '}' in result_text:
#                 # æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
#                 start = result_text.find('{')
#                 brace_count = 0
#                 end = start
#                 for i, char in enumerate(result_text[start:], start):
#                     if char == '{':
#                         brace_count += 1
#                     elif char == '}':
#                         brace_count -= 1
#                         if brace_count == 0:
#                             end = i + 1
#                             break
#                 json_text = result_text[start:end]
#             else:
#                 json_text = result_text.strip()
            
#             # æ¸…ç†å¯èƒ½çš„å¤šä½™å†…å®¹
#             json_text = json_text.strip()
#             if json_text.endswith('```'):
#                 json_text = json_text[:-3].strip()
                
#             logger.debug(f"è¡Œ {row_idx} æå–çš„JSON: {json_text[:200]}...")
            
#             result = json.loads(json_text)
#             logger.info(f"è¡Œ {row_idx} JSONè§£ææˆåŠŸ")
            
#         except json.JSONDecodeError as e:
#             logger.error(f"è¡Œ {row_idx} JSONè§£æå¤±è´¥: {e}")
#             logger.error(f"æå–çš„JSONæ–‡æœ¬: {json_text[:500]}...")
            
#             # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
#             try:
#                 # ç§»é™¤å¯èƒ½çš„æ³¨é‡Š
#                 import re
#                 cleaned_json = re.sub(r'//.*?\n', '', json_text)
#                 cleaned_json = re.sub(r'/\*.*?\*/', '', cleaned_json, flags=re.DOTALL)
#                 result = json.loads(cleaned_json)
#                 logger.info(f"è¡Œ {row_idx} JSONä¿®å¤åè§£ææˆåŠŸ")
#             except:
#                 logger.error(f"è¡Œ {row_idx} JSONä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
#                 # ä½¿ç”¨é»˜è®¤å€¼
#                 result = {
#                     "quality_rating": {
#                         "overall": "ERROR",
#                         "detailed_scores": {"error": f"JSONè§£æå¤±è´¥: {str(e)}"}
#                     }
#                 }
#         except Exception as e:
#             logger.error(f"è¡Œ {row_idx} å¤„ç†å¼‚å¸¸: {e}")
#             result = {
#                 "quality_rating": {
#                     "overall": "ERROR", 
#                     "detailed_scores": {"error": f"å¤„ç†å¼‚å¸¸: {str(e)}"}
#                 }
#             }
        
#         # 5. æ›´æ–°è¡Œæ•°æ®
#         row_data['quality_rating'] = result['quality_rating']['overall']
#         row_data['detailed_scores'] = str(result['quality_rating']['detailed_scores'])
#         logger.info(f"è¡Œ {row_idx} å¤„ç†å®Œæˆï¼Œè¯„åˆ†: {row_data['quality_rating']}")
#         return (row_idx, row_data)

#     except Exception as e:
#         logger.error(f"å¤„ç†è¡Œ {row_idx} æ—¶å‡ºé”™: {str(e)}")
#         # ä¿ç•™åŸå§‹è¡Œæ•°æ®å¹¶æ ‡è®°é”™è¯¯
#         row_data['quality_rating'] = "ERROR"
#         row_data['detailed_scores'] = str(e)
#         return (row_idx, row_data)

# if __name__ == '__main__':
#     # 0. æœåŠ¡å™¨é…ç½®
#     server_config = {
#         "base_url": "https://ark.cn-beijing.volces.com/api/v3",
#         "api_key": "5a032496-1268-4e6f-b6ee-a9affc6b5469",
#         "model_name": "ep-20250813144949-kchv2"
#     }

#     # 1. åŠ è½½æºæ•°æ®
#     file_name = "/mnt/workspace/LLM/ldd/sft/result/Qwen325ç»“æœ.xlsx"
#     data = pd.read_excel(file_name)
#     logger.info(f"åŠ è½½æ•°æ®å®Œæˆï¼Œå…± {data.shape[0]} è¡Œ")

#     # æ£€æŸ¥æ•°æ®åˆ‡ç‰‡ - å¤„ç†æ‰€æœ‰æ•°æ®
#     original_data = data.copy()
#     data = data.copy()  # å¤„ç†å…¨éƒ¨æ•°æ®
#     logger.info(f"å‡†å¤‡å¤„ç†æ•°æ®: {data.shape[0]} è¡Œ")
    
#     # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨ï¼ˆç§»é™¤æ€ç»´é“¾æ£€æŸ¥ï¼‰
#     required_columns = ['é—®é¢˜', 'ç­”æ¡ˆ']
#     missing_columns = [col for col in required_columns if col not in data.columns]
#     if missing_columns:
#         logger.error(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
#         logger.info(f"æ•°æ®åˆ—å: {list(data.columns)}")
#         exit(1)

#     # 2. å‡†å¤‡å¤šè¿›ç¨‹å¤„ç†
#     num_workers = min(5, data.shape[0])  # å‡å°‘è¿›ç¨‹æ•°ï¼Œé¿å…è¿‡å¤šç©ºé—²è¿›ç¨‹
#     logger.info(f"å¯ç”¨ {num_workers} ä¸ªå·¥ä½œè¿›ç¨‹")

#     # å…ˆæµ‹è¯•å•ä¸ªè¯·æ±‚æ˜¯å¦æ­£å¸¸ï¼ˆç§»é™¤æ€ç»´é“¾å‚æ•°ï¼‰
#     if data.shape[0] > 0:
#         test_row = data.iloc[0]
#         logger.info("æµ‹è¯•å•ä¸ªè¯·æ±‚...")
#         try:
#             test_result = evaluate_qa_quality(
#                 test_row['é—®é¢˜'], 
#                 test_row['ç­”æ¡ˆ']
#             )
#             logger.info(f"æµ‹è¯•è¯·æ±‚æˆåŠŸ: {test_result[:100]}...")
#         except Exception as e:
#             logger.error(f"æµ‹è¯•è¯·æ±‚å¤±è´¥: {str(e)}")
#             exit(1)

#     # 3. åˆ›å»ºè¿›ç¨‹æ± 
#     try:
#         with multiprocessing.Pool(processes=num_workers) as pool:
#             # å‡†å¤‡ä»»åŠ¡å‚æ•° [(è¡Œç´¢å¼•, è¡Œæ•°æ®, æœåŠ¡å™¨é…ç½®), ...]
#             task_args = [(idx, row.to_dict(), server_config) for idx, row in data.iterrows()]
#             logger.info(f"å‡†å¤‡å¤„ç† {len(task_args)} ä¸ªä»»åŠ¡")

#             # 4. å¹¶è¡Œå¤„ç†å¹¶æ”¶é›†ç»“æœ
#             results = []
#             completed_count = 0
#             for result in tqdm(pool.imap(process_row, task_args), total=len(task_args), desc="å¤„ç†è¿›åº¦"):
#                 results.append(result)
#                 completed_count += 1
#                 if completed_count % 1 == 0:  # æ¯å®Œæˆ1ä¸ªå°±æ‰“å°ä¸€æ¬¡
#                     logger.info(f"å·²å®Œæˆ {completed_count}/{len(task_args)} ä¸ªä»»åŠ¡")

#         logger.info(f"æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(results)} ä¸ªç»“æœ")
        
#     except Exception as e:
#         logger.error(f"å¤šè¿›ç¨‹å¤„ç†å‡ºé”™: {str(e)}")
#         exit(1)

#     # 5. æŒ‰åŸå§‹é¡ºåºæ›´æ–°æ•°æ®
#     logger.info("å¼€å§‹æ›´æ–°ç»“æœæ•°æ®")
#     updated_count = 0
#     for row_idx, updated_row in results:
#         if row_idx in data.index:
#             data.loc[row_idx,'quality_rating'] = updated_row['quality_rating']
#             data.loc[row_idx, 'detailed_scores'] = updated_row['detailed_scores']
#             updated_count += 1
#         else:
#             logger.warning(f"ç´¢å¼• {row_idx} ä¸åœ¨æ•°æ®ä¸­")
    
#     logger.info(f"æˆåŠŸæ›´æ–° {updated_count} è¡Œæ•°æ®")

#     # 6. ä¿å­˜ç»“æœ
#     save_dir = './result'
#     save_name = f'å¢å¼ºå‰çš„è¯„ä¼°ç»“æœ_ç«å±±ç‰ˆ_æ— æ€ç»´é“¾_{time_label}.xlsx'
#     save_path = os.path.join(save_dir, save_name)

#     os.makedirs(save_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨

#     # ç»Ÿè®¡è´¨é‡è¯„åˆ†åˆ†å¸ƒ
#     if 'quality_rating' in data.columns:
#         quality_counts = data['quality_rating'].value_counts()
#         logger.info("=" * 50)
#         logger.info("è´¨é‡è¯„åˆ†ç»Ÿè®¡ç»“æœ:")
#         logger.info("=" * 50)
        
#         high_count = quality_counts.get('high', 0)
#         medium_count = quality_counts.get('medium', 0)
#         low_count = quality_counts.get('low', 0)
#         error_count = quality_counts.get('ERROR', 0)
        
#         total_evaluated = high_count + medium_count + low_count
        
#         logger.info(f"ğŸ“Š HIGH (é«˜è´¨é‡):   {high_count:4d} æ¡ ({high_count/len(data)*100:.1f}%)")
#         logger.info(f"ğŸ“Š MEDIUM (ä¸­ç­‰è´¨é‡): {medium_count:4d} æ¡ ({medium_count/len(data)*100:.1f}%)")
#         logger.info(f"ğŸ“Š LOW (ä½è´¨é‡):    {low_count:4d} æ¡ ({low_count/len(data)*100:.1f}%)")
#         if error_count > 0:
#             logger.info(f"âŒ ERROR (å¤„ç†å¤±è´¥): {error_count:4d} æ¡ ({error_count/len(data)*100:.1f}%)")
#         logger.info(f"âœ… æ€»è®¡å·²è¯„ä¼°:      {total_evaluated:4d} æ¡")
#         logger.info("=" * 50)
        
#         # ä¿å­˜ç»Ÿè®¡æ‘˜è¦ï¼ˆä¿®å¤JSONåºåˆ—åŒ–é—®é¢˜ï¼‰
#         summary_stats = {
#             'total_samples': int(len(data)),
#             'high_quality': int(high_count),
#             'medium_quality': int(medium_count),
#             'low_quality': int(low_count),
#             'error_count': int(error_count),
#             'high_percentage': float(round(high_count/len(data)*100, 2)),
#             'medium_percentage': float(round(medium_count/len(data)*100, 2)),
#             'low_percentage': float(round(low_count/len(data)*100, 2))
#         }
        
#         # ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°JSON
#         stats_path = os.path.join(save_dir, f'è´¨é‡è¯„ä¼°ç»Ÿè®¡_ç«å±±ç‰ˆ_æ— æ€ç»´é“¾_{time_label}.json')
#         with open(stats_path, 'w', encoding='utf-8') as f:
#             json.dump(summary_stats, f, ensure_ascii=False, indent=2)
#         logger.info(f"ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {stats_path}")
#     else:
#         logger.warning("æœªæ‰¾åˆ°quality_ratingåˆ—ï¼Œæ— æ³•ç»Ÿè®¡")

#     # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
#     if data.empty:
#         logger.error("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
#         exit(1)
        
#     try:
#         with pd.ExcelWriter(save_path) as writer:
#             data.to_excel(writer, index=False)
#         logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
#         logger.info(f"ä¿å­˜çš„æ•°æ®å½¢çŠ¶: {data.shape}")
#     except Exception as e:
#         logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
#         # å°è¯•ä¿å­˜ä¸ºCSVä½œä¸ºå¤‡é€‰
#         csv_path = save_path.replace('.xlsx', '.csv')
#         data.to_csv(csv_path, index=False)
#         logger.info(f"å·²ä¿å­˜ä¸ºCSVæ ¼å¼: {csv_path}")